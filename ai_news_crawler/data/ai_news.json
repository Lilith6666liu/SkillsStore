[
  {
    "id": "75d4500086da59c53e5718667232ef26",
    "title": "è±†åŒ…çš„æ–°èº«ä»½æ›å…‰ï¼šåœ¨å›½é™…è‰ºæœ¯å±•å½“èµ·äº†â€œAIè®²è§£å‘˜â€",
    "url": "https://www.qbitai.com/2026/01/370665.html",
    "source_id": "qbitai",
    "source_name": "é‡å­ä½",
    "language": "zh",
    "category": "news",
    "summary": "",
    "tags": [
      "èµ„è®¯",
      "é¦–é¡µè½®æ’­",
      "è±†åŒ…"
    ],
    "publish_time": "2026-01-20T10:35:32",
    "fetch_time": "2026-01-22T09:14:25.419576"
  },
  {
    "id": "6e836bbd7837c9fef7a993cdbad785bc",
    "title": "ä¸­å›½å›¢é˜Ÿé¦–æ¬¡åœ¨Natureå­åˆŠå‘å¸ƒåŒ»ç–—AIæ ‡å‡†ï¼Œæœªæ¥åŒ»ç”ŸMedGPTæ‘˜å¾—å…¨çƒæ¡‚å† ",
    "url": "https://www.qbitai.com/2026/01/370734.html",
    "source_id": "qbitai",
    "source_name": "é‡å­ä½",
    "language": "zh",
    "category": "news",
    "summary": "ä¸´åºŠçº§åŒ»ç–—AIæ–°æ ‡å°º",
    "tags": [
      "MedGPT",
      "æœªæ¥åŒ»ç”Ÿ",
      "åŒ»ç–—AI",
      "é¦–é¡µè½®æ’­",
      "GPT",
      "èµ„è®¯",
      "Nature"
    ],
    "publish_time": "2026-01-21T04:13:21",
    "fetch_time": "2026-01-22T09:14:25.419523"
  },
  {
    "id": "533bf89e3d73a926f71a73218f6ebb10",
    "title": "é©¬æ–¯å…‹ç½•è§ä½Žå¤´ï¼šå¼€æºð•æŽ¨èç®—æ³•ï¼Œè‡ªå˜²â€œå¾ˆçƒ‚â€ä¸è¿‡æœªæ¥æœˆæ›´",
    "url": "https://www.qbitai.com/2026/01/370760.html",
    "source_id": "qbitai",
    "source_name": "é‡å­ä½",
    "language": "zh",
    "category": "news",
    "summary": "åŽŸæ¥çº¯AIé©±åŠ¨çš„æŽ¨èç³»ç»Ÿï¼Œæ˜¯è¿™æ ·è¿ä½œçš„ï¼",
    "tags": [
      "èµ„è®¯",
      "é©¬æ–¯å…‹"
    ],
    "publish_time": "2026-01-21T04:32:45",
    "fetch_time": "2026-01-22T09:14:25.419478"
  },
  {
    "id": "1227b1f11776839607cc2c3204726a55",
    "title": "å¾®è½¯æ‰“åŒ…æ”¶è´­OpenAIï¼Ÿå°±å·®ä¸€ç‚¹ï¼",
    "url": "https://www.qbitai.com/2026/01/370907.html",
    "source_id": "qbitai",
    "source_name": "é‡å­ä½",
    "language": "zh",
    "category": "news",
    "summary": "èµ„é‡‘ã€æ³•å¾‹æ–‡ä¹¦å…¨åˆ°ä½ï¼Œè¿žåå­—éƒ½å·²æƒ³å¥½",
    "tags": [
      "èµ„è®¯",
      "é¦–é¡µè½®æ’­",
      "OpenAI",
      "å¾®è½¯"
    ],
    "publish_time": "2026-01-21T08:32:53",
    "fetch_time": "2026-01-22T09:14:25.419383"
  },
  {
    "id": "555829c080814e4ca9afef75f3063fc6",
    "title": "2026å¹´OpenAIæœ€çœ‹å¥½çš„3ä¸ªæ–¹å‘",
    "url": "https://www.qbitai.com/2026/01/370936.html",
    "source_id": "qbitai",
    "source_name": "é‡å­ä½",
    "language": "zh",
    "category": "news",
    "summary": "ç®—åŠ›ä¸Žæ”¶å…¥ä¹‹é—´å­˜åœ¨æ˜Žæ˜¾æ­£ç›¸å…³",
    "tags": [
      "èµ„è®¯",
      "OpenAI",
      "æ™ºèƒ½ä½“"
    ],
    "publish_time": "2026-01-21T08:52:13",
    "fetch_time": "2026-01-22T09:14:25.419337"
  },
  {
    "id": "9ef2f40a370599cb44770201c935b104",
    "title": "çªå‘ï¼xAIè”åˆ›æ¨æ ¼è¿‡åŠ³ç—…ç¦»èŒï¼Œç»™é©¬æ–¯å…‹å¹²æ´»åŽ‹åŠ›å±±å¤§",
    "url": "https://www.qbitai.com/2026/01/370985.html",
    "source_id": "qbitai",
    "source_name": "é‡å­ä½",
    "language": "zh",
    "category": "news",
    "summary": "",
    "tags": [
      "èµ„è®¯",
      "xAI",
      "é©¬æ–¯å…‹"
    ],
    "publish_time": "2026-01-21T09:11:46",
    "fetch_time": "2026-01-22T09:14:25.419285"
  },
  {
    "id": "4cd2f9b971762708431d57b3272953c1",
    "title": "xAIå·¥ç¨‹å¸ˆæ’­å®¢èŠå¤ªå—¨ï¼Œé©¬æ–¯å…‹è§£é›‡äº†ä»–",
    "url": "https://www.qbitai.com/2026/01/371158.html",
    "source_id": "qbitai",
    "source_name": "é‡å­ä½",
    "language": "zh",
    "category": "news",
    "summary": "ã€Œå·¨ç¡¬ã€é¡¹ç›®é¦–æ¬¡å…¬å¼€",
    "tags": [
      "èµ„è®¯",
      "é¦–é¡µè½®æ’­",
      "xAI"
    ],
    "publish_time": "2026-01-21T10:07:41",
    "fetch_time": "2026-01-22T09:14:25.419270"
  },
  {
    "id": "9bb5b95df8461c196e801c73dbfc4aae",
    "title": "å½“é»„ä»å‹‹å°†å­˜å‚¨å®šä¹‰ä¸ºã€ŒAIè¿è¡Œå†…å­˜ã€ï¼ŒåŸºç¡€è®¾æ–½è¯¥å¦‚ä½•å®žçŽ°ç‰©ç§è¿›åŒ–ï¼Ÿ",
    "url": "https://www.jiqizhixin.com/articles/2026-01-20-11",
    "source_id": "jiqizhixin",
    "source_name": "æœºå™¨ä¹‹å¿ƒ",
    "language": "zh",
    "category": "news",
    "summary": "",
    "tags": [],
    "publish_time": "2026-01-20T10:35:39",
    "fetch_time": "2026-01-22T09:14:24.192352"
  },
  {
    "id": "279f4a05efce7e0bdaf88131a8424882",
    "title": "å‡»è´¥GPTã€Geminiï¼Œå¤æ—¦Ã—åˆ›æ™ºå­µåŒ–åˆ›ä¸šå›¢é˜Ÿã€Œæ¨¡æ€æ™ºèƒ½ã€ï¼Œè¯­éŸ³æ¨¡åž‹ä¸Šæ–°äº†",
    "url": "https://www.jiqizhixin.com/articles/2026-01-20-12",
    "source_id": "jiqizhixin",
    "source_name": "æœºå™¨ä¹‹å¿ƒ",
    "language": "zh",
    "category": "news",
    "summary": "",
    "tags": [
      "GPT"
    ],
    "publish_time": "2026-01-20T10:40:00",
    "fetch_time": "2026-01-22T09:14:24.192345"
  },
  {
    "id": "b578a05210ad2933fe2bc7769879e436",
    "title": "é©¬æ–¯å…‹åˆšåˆšçœŸæŠŠ ð• å¹³å°æŽ¨èç®—æ³•ç»™å¼€æºäº†ï¼Œæ ¸å¿ƒä¹Ÿæ˜¯Transformer",
    "url": "https://www.jiqizhixin.com/articles/2026-01-20-13",
    "source_id": "jiqizhixin",
    "source_name": "æœºå™¨ä¹‹å¿ƒ",
    "language": "zh",
    "category": "news",
    "summary": "",
    "tags": [
      "Transformer"
    ],
    "publish_time": "2026-01-20T12:44:24",
    "fetch_time": "2026-01-22T09:14:24.192337"
  },
  {
    "id": "733654754318b82048b2e39fccf772de",
    "title": "AAAI 2026 Oral | å‘Šåˆ«æ³¨æ„åŠ›ä¸Žçƒ­ä¼ å¯¼ï¼åŒ—å¤§æ¸…åŽæå‡ºWaveFormerï¼Œé¦–åˆ›æ³¢åŠ¨æ–¹ç¨‹å»ºæ¨¡è§†è§‰",
    "url": "https://www.jiqizhixin.com/articles/2026-01-21",
    "source_id": "jiqizhixin",
    "source_name": "æœºå™¨ä¹‹å¿ƒ",
    "language": "zh",
    "category": "news",
    "summary": "",
    "tags": [],
    "publish_time": "2026-01-21T02:17:29",
    "fetch_time": "2026-01-22T09:14:24.192322"
  },
  {
    "id": "32c133187f2b4b4041ee24edcefdcde9",
    "title": "AI5èŠ¯ç‰‡æžå®šï¼Œé©¬æ–¯å…‹çš„çº¯è‡ªç ”è¶…ç®—Dojo 3åˆå›žæ¥äº†",
    "url": "https://www.jiqizhixin.com/articles/2026-01-21-3",
    "source_id": "jiqizhixin",
    "source_name": "æœºå™¨ä¹‹å¿ƒ",
    "language": "zh",
    "category": "news",
    "summary": "",
    "tags": [],
    "publish_time": "2026-01-21T05:15:32",
    "fetch_time": "2026-01-22T09:14:24.192306"
  },
  {
    "id": "d408eda45e5f63c14cd9d53677436d42",
    "title": "AI for Scienceå¼€å¹´æ–°çªç ´ï¼šä¸­ç§‘å¤§å®žçŽ°å¤šå°ºåº¦ç»“æž„é€†å‘è®¾è®¡128å€åŠ é€Ÿï¼Œç™»ä¸ŠNatureå­åˆŠ",
    "url": "https://www.jiqizhixin.com/articles/2026-01-21-6",
    "source_id": "jiqizhixin",
    "source_name": "æœºå™¨ä¹‹å¿ƒ",
    "language": "zh",
    "category": "news",
    "summary": "",
    "tags": [],
    "publish_time": "2026-01-21T09:58:19",
    "fetch_time": "2026-01-22T09:14:24.192283"
  },
  {
    "id": "9196b6a10cfac59a2164c8e9b0b5934b",
    "title": "éžTransformeræž¶æž„çš„æ–°çªç ´ï¼Œæ¶²æ€ç¥žç»ç½‘ç»œçš„æŽ¨ç†å°æ¨¡åž‹åªç”¨900Må†…å­˜",
    "url": "https://www.jiqizhixin.com/articles/2026-01-21-7",
    "source_id": "jiqizhixin",
    "source_name": "æœºå™¨ä¹‹å¿ƒ",
    "language": "zh",
    "category": "news",
    "summary": "",
    "tags": [
      "Transformer"
    ],
    "publish_time": "2026-01-21T10:02:59",
    "fetch_time": "2026-01-22T09:14:24.192274"
  },
  {
    "id": "023feac0a164208c10a4313eee1726b6",
    "title": "Scale creative asset discovery with Amazon Nova Multimodal Embeddings unified vector search",
    "url": "https://aws.amazon.com/blogs/machine-learning/scale-creative-asset-discovery-with-amazon-nova-multimodal-embeddings-unified-vector-search/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "In this post, we describe how you can use Amazon Nova Multimodal Embeddings to retrieve specific video segments. We also review a real-world use case in which Nova Multimodal Embeddings achieved a recall success rate of 96.7% and a high-precision recall of 73.3% (returning the target content in the top two results) when tested against a library of 170 gaming creative assets. The model also demonstrates strong cross-language capabilities with minimal performance degradation across multiple langua",
    "tags": [
      "Intermediate (200)",
      "Reinforcement Learning",
      "Amazon Nova",
      "Computer Vision",
      "Amazon Machine Learning",
      "Amazon Bedrock",
      "Artificial Intelligence"
    ],
    "publish_time": "2026-01-15T15:45:02",
    "fetch_time": "2026-01-22T09:14:19.518281"
  },
  {
    "id": "d604398a211048d6a28367df9003190c",
    "title": "Safeguard generative AI applications with Amazon Bedrock Guardrails",
    "url": "https://aws.amazon.com/blogs/machine-learning/safeguard-generative-ai-applications-with-amazon-bedrock-guardrails/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "In this post, we demonstrate how you can address these challenges by adding centralized safeguards to a custom multi-provider generative AI gateway using Amazon Bedrock Guardrails.",
    "tags": [
      "Amazon Machine Learning",
      "Amazon Bedrock AgentCore",
      "AI/ML",
      "Amazon Bedrock",
      "Amazon Bedrock Guardrails",
      "Artificial Intelligence"
    ],
    "publish_time": "2026-01-15T15:50:54",
    "fetch_time": "2026-01-22T09:14:19.518252"
  },
  {
    "id": "44c2c2470a47dc6ccfe5f570f93d023a",
    "title": "Build a generative AI-powered business reporting solution with Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-a-generative-ai-powered-business-reporting-solution-with-amazon-bedrock/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "This post introduces generative AI guided business reportingâ€”with a focus on writing achievements & challenges about your businessâ€”providing a smart, practical solution that helps simplify and accelerate internal communication and reporting.",
    "tags": [
      "Amazon Machine Learning",
      "AI/ML",
      "Amazon Bedrock",
      "Amazon Bedrock Guardrails",
      "Technical How-to",
      "Generative AI"
    ],
    "publish_time": "2026-01-15T15:53:15",
    "fetch_time": "2026-01-22T09:14:19.518221"
  },
  {
    "id": "479a3b0dbecfd5f644e6efd9e6061dda",
    "title": "How the Amazon AMET Payments team accelerates test case generation with Strands Agents",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-the-amazon-amet-payments-team-accelerates-test-case-generation-with-strands-agents/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "In this post, we explain how we overcame the limitations of single-agent AI systems through a human-centric approach, implemented structured outputs to significantly reduce hallucinations and built a scalable solution now positioned for expansion across the AMET QA team and later across other QA teams in International Emerging Stores and Payments (IESP) Org.",
    "tags": [
      "Customer Solutions",
      "AI Agent",
      "Best Practices",
      "Python",
      "Amazon Machine Learning",
      "Financial Services",
      "Strands Agents",
      "AI/ML",
      "Amazon Bedrock",
      "Generative AI",
      "Technical How-to",
      "Artificial Intelligence",
      "Generative AI*",
      "Industries"
    ],
    "publish_time": "2026-01-15T15:55:35",
    "fetch_time": "2026-01-22T09:14:19.518166"
  },
  {
    "id": "950e29cb1a544f5d0bec49793eb74a22",
    "title": "Deploy AI agents on Amazon Bedrock AgentCore using GitHub Actions",
    "url": "https://aws.amazon.com/blogs/machine-learning/deploy-ai-agents-on-amazon-bedrock-agentcore-using-github-actions/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "In this post, we demonstrate how to use a GitHub Actions workflow to automate the deployment of AI agents on AgentCore Runtime. This approach delivers a scalable solution with enterprise-level security controls, providing complete continuous integration and delivery (CI/CD) automation.",
    "tags": [
      "AI Agent",
      "Best Practices",
      "Strands Agents",
      "Amazon Machine Learning",
      "Amazon Bedrock AgentCore",
      "Advanced (300)",
      "Technical How-to"
    ],
    "publish_time": "2026-01-16T15:37:37",
    "fetch_time": "2026-01-22T09:14:19.518131"
  },
  {
    "id": "245f6153d347f03df446e65c3da993cf",
    "title": "From beginner to champion: A studentâ€™s journey through the AWS AI League ASEAN finals",
    "url": "https://aws.amazon.com/blogs/machine-learning/from-beginner-to-champion-a-students-journey-through-the-aws-ai-league-asean-finals/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "The AWS AI League, launched by Amazon Web Services (AWS), expanded its reach to the Association of Southeast Asian Nations (ASEAN) last year, welcoming student participants from Singapore, Indonesia, Malaysia, Thailand, Vietnam, and the Philippines. In this blog post, youâ€™ll hear directly from the AWS AI League champion, Blix D. Foryasen, as he shares his reflection on the challenges, breakthroughs, and key lessons discovered throughout the competition.",
    "tags": [
      "Amazon Bedrock",
      "Customer Solutions",
      "Amazon SageMaker JumpStart"
    ],
    "publish_time": "2026-01-16T15:41:55",
    "fetch_time": "2026-01-22T09:14:19.518102"
  },
  {
    "id": "85e73c8039529e946f9764c571d64881",
    "title": "How Palo Alto Networks enhanced device security infra log analysis with Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-palo-alto-networks-enhanced-device-security-infra-log-analysis-with-amazon-bedrock/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "Palo Alto Networksâ€™ Device Security team wanted to detect early warning signs of potential production issues to provide more time to SMEs to react to these emerging problems. They partnered with the AWS Generative AI Innovation Center (GenAIIC) to develop an automated log classification pipeline powered by Amazon Bedrock. In this post, we discuss how Amazon Bedrock, through Anthropicâ€™ s Claude Haiku model, and Amazon Titan Text Embeddings work together to automatically classify and analyze log d",
    "tags": [
      "Customer Solutions",
      "Reinforcement Learning",
      "Amazon Titan",
      "Amazon Aurora",
      "Amazon Bedrock",
      "Amazon Redshift",
      "Technical How-to"
    ],
    "publish_time": "2026-01-16T15:46:36",
    "fetch_time": "2026-01-22T09:14:19.518074"
  },
  {
    "id": "754a04bd6851180e6e86ac293acbd03f",
    "title": "Advanced fine-tuning techniques for multi-agent orchestration: Patterns from Amazon at scale",
    "url": "https://aws.amazon.com/blogs/machine-learning/advanced-fine-tuning-techniques-for-multi-agent-orchestration-patterns-from-amazon-at-scale/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "In this post, we show you how fine-tuning enabled a 33% reduction in dangerous medication errors (Amazon Pharmacy), engineering 80% human effort reduction (Amazon Global Engineering Services), and content quality assessments improving 77% to 96% accuracy (Amazon A+). This post details the techniques behind these outcomes: from foundational methods like Supervised Fine-Tuning (SFT) (instruction tuning), and Proximal Policy Optimization (PPO), to Direct Preference Optimization (DPO) for human alig",
    "tags": [
      "Intermediate (200)",
      "Amazon SageMaker AI",
      "AI Agent",
      "Best Practices",
      "Generative AI",
      "Thought Leadership"
    ],
    "publish_time": "2026-01-16T15:51:21",
    "fetch_time": "2026-01-22T09:14:19.518042"
  },
  {
    "id": "e228cfc9387f043d8799486ec450d2f2",
    "title": "Using Strands Agents to create a multi-agent solution with Metaâ€™s Llama 4 and Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/using-strands-agents-to-create-a-multi-agent-solution-with-metas-llama-4-and-amazon-bedrock/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "In this post, we explore how to build a multi-agent video processing workflow using Strands Agents, Meta's Llama 4 models, and Amazon Bedrock to automatically analyze and understand video content through specialized AI agents working in coordination. To showcase the solution, we will use Amazon SageMaker AI to walk you through the code.",
    "tags": [
      "AI Agent",
      "Computer Vision",
      "Strands Agents",
      "Amazon Machine Learning",
      "AI/ML",
      "Amazon Bedrock",
      "Generative AI",
      "Technical How-to",
      "Artificial Intelligence",
      "AIML"
    ],
    "publish_time": "2026-01-21T17:47:44",
    "fetch_time": "2026-01-22T09:14:19.517981"
  },
  {
    "id": "5d9c2c97847e86f4f084ce3e64c004ed",
    "title": "How bunq handles 97% of support with Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-bunq-handles-97-of-support-with-amazon-bedrock/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "In this post, we show how bunq upgraded Finn, its in-house generative AI assistant, using Amazon Bedrock to transform user support and banking operations to be seamless, in multiple languages and time zones.",
    "tags": [
      "Customer Solutions",
      "Machine Learning",
      "Amazon Machine Learning",
      "Amazon Bedrock",
      "Generative AI"
    ],
    "publish_time": "2026-01-21T17:50:35",
    "fetch_time": "2026-01-22T09:14:19.517944"
  },
  {
    "id": "6f6f99bdaa7fa96cccbbfdb3b898fed9",
    "title": "How Thomson Reuters built an Agentic Platform Engineering Hub with Amazon Bedrock AgentCore",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-thomson-reuters-built-an-agentic-platform-engineering-hub-with-amazon-bedrock-agentcore/",
    "source_id": "aws_ml",
    "source_name": "AWS Machine Learning Blog",
    "language": "en",
    "category": "tutorial",
    "summary": "This blog post explains how TR's Platform Engineering team, a geographically distributed unit overseeing TR's service availability, boosted its operational productivity by transitioning from manual to an automated agentic system using Amazon Bedrock AgentCore.",
    "tags": [
      "Customer Solutions",
      "AI Agent",
      "Amazon Machine Learning",
      "Amazon Bedrock AgentCore",
      "AI/ML",
      "Amazon Bedrock",
      "Generative AI",
      "AWS Customer",
      "Artificial Intelligence"
    ],
    "publish_time": "2026-01-21T21:39:42",
    "fetch_time": "2026-01-22T09:14:19.517853"
  },
  {
    "id": "d7d98f1a39e66feca45f3b70c77b1c5e",
    "title": "TWeddit : A Dataset of Triggering Stories Predominantly Shared by Women on Reddit",
    "url": "https://arxiv.org/abs/2601.11819",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11819v1 Announce Type: new \nAbstract: Warning: This paper may contain examples and topics that may be disturbing to some readers, especially survivors of miscarriage and sexual violence. People affected by abortion, miscarriage, or sexual violence often share their experiences on social media to express emotions and seek support. On public platforms like Reddit, where users can post long, detailed narratives (up to 40,000 characters), readers may be exposed to distressing content. Alt",
    "tags": [
      "cs.CL"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.211232"
  },
  {
    "id": "dd55be760330f946ee6aaa11ec378df5",
    "title": "Beyond Tokens: Concept-Level Training Objectives for LLMs",
    "url": "https://arxiv.org/abs/2601.11791",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11791v1 Announce Type: new \nAbstract: The next-token prediction (NTP) objective has been foundational in the development of modern large language models (LLMs), driving advances in fluency and generalization. However, NTP operates at the \\textit{token} level, treating deviations from a single reference continuation as errors even when alternative continuations are equally plausible or semantically equivalent (e.g., ``mom'' vs. ``mother''). As a result, token-level loss can penalize va",
    "tags": [
      "cs.CL",
      "LLM"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.211202"
  },
  {
    "id": "8890c821adfa9ef48b50b4ebc09dc26c",
    "title": "Translation as a Scalable Proxy for Multilingual Evaluation",
    "url": "https://arxiv.org/abs/2601.11778",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11778v1 Announce Type: new \nAbstract: The rapid proliferation of LLMs has created a critical evaluation paradox: while LLMs claim multilingual proficiency, comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving >98% of the world's 7,000 languages in an empirical void. Traditional benchmark construction faces scaling challenges such as cost, scarcity of domain experts, and data contamination. We evaluate the validity of a simpler alternative: can tra",
    "tags": [
      "cs.CL",
      "cs.AI",
      "LLM",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.211170"
  },
  {
    "id": "e768013b0ebaa73020021938a751b6f8",
    "title": "Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models",
    "url": "https://arxiv.org/abs/2601.11776",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11776v1 Announce Type: new \nAbstract: Recent breakthroughs in Large Language Models (LLMs) have revealed remarkable generative capabilities and emerging self-regulatory mechanisms, including self-correction and self-rewarding. However, current detoxification techniques rarely exploit these built-in abilities; instead, they rely on external modules, labor-intensive data annotation, or human intervention --factors that hinder scalability and consistency. In this paper, we introduce a fu",
    "tags": [
      "cs.CL",
      "cs.AI",
      "LLM"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.211138"
  },
  {
    "id": "fd27133041898ec1fc7e648cdda20ee7",
    "title": "Early Linguistic Pattern of Anxiety from Social Media Using Interpretable Linguistic Features: A Multi-Faceted Validation Study with Author-Disjoint Evaluation",
    "url": "https://arxiv.org/abs/2601.11758",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11758v1 Announce Type: new \nAbstract: Anxiety affects hundreds of millions of individuals globally, yet large-scale screening remains limited. Social media language provides an opportunity for scalable detection, but current models often lack interpretability, keyword-robustness validation, and rigorous user-level data integrity. This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-do",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.211069"
  },
  {
    "id": "8a767c89c402fb2a915745c5c3576755",
    "title": "LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text",
    "url": "https://arxiv.org/abs/2601.11746",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11746v1 Announce Type: new \nAbstract: Local explanation methods such as LIME (Ribeiro et al., 2016) remain fundamental to trustworthy AI, yet their application to NLP is limited by a reliance on random token masking. These heuristic perturbations frequently generate semantically invalid, out-of-distribution inputs that weaken the fidelity of local surrogate models. While recent generative approaches such as LLiMe (Angiulli et al., 2025b) attempt to mitigate this by employing Large Lan",
    "tags": [
      "cs.LG",
      "cs.AI",
      "LLM",
      "cs.CL",
      "NLP"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.211031"
  },
  {
    "id": "e38eac371f2eb95f89ca9043e11ff8a4",
    "title": "Bridging Human Interpretation and Machine Representation: A Landscape of Qualitative Data Analysis in the LLM Era",
    "url": "https://arxiv.org/abs/2601.11739",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11739v1 Announce Type: new \nAbstract: LLMs are increasingly used to support qualitative research, yet existing systems produce outputs that vary widely--from trace-faithful summaries to theory-mediated explanations and system models. To make these differences explicit, we introduce a 4$\\times$4 landscape crossing four levels of meaning-making (descriptive, categorical, interpretive, theoretical) with four levels of modeling (static structure, stages/timelines, causal pathways, feedbac",
    "tags": [
      "cs.CL",
      "LLM"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210996"
  },
  {
    "id": "c22a949aa0be3ec91b740272cb5810f9",
    "title": "RAC: Retrieval-Augmented Clarification for Faithful Conversational Search",
    "url": "https://arxiv.org/abs/2601.11722",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11722v1 Announce Type: new \nAbstract: Clarification questions help conversational search systems resolve ambiguous or underspecified user queries. While prior work has focused on fluency and alignment with user intent, especially through facet extraction, much less attention has been paid to grounding clarifications in the underlying corpus. Without such grounding, systems risk asking questions that cannot be answered from the available documents. We introduce RAC (Retrieval-Augmented",
    "tags": [
      "cs.CL",
      "Reinforcement Learning",
      "cs.IR"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210966"
  },
  {
    "id": "a899a4169346b2c9921509701d7c3ee9",
    "title": "Towards AGI A Pragmatic Approach Towards Self Evolving Agent",
    "url": "https://arxiv.org/abs/2601.11658",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11658v1 Announce Type: new \nAbstract: Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a tas",
    "tags": [
      "AI Agent",
      "cs.AI",
      "LLM",
      "cs.CL",
      "AGI"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210934"
  },
  {
    "id": "dda5df246933f3245d238edc2f622d80",
    "title": "Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents",
    "url": "https://arxiv.org/abs/2601.11585",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11585v1 Announce Type: new \nAbstract: Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually",
    "tags": [
      "cs.CL",
      "LLM",
      "Reinforcement Learning",
      "AI Agent"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210902"
  },
  {
    "id": "9bde71f91f16f457f027eb4728d2b37e",
    "title": "Enhancing the QA Model through a Multi-domain Debiasing Framework",
    "url": "https://arxiv.org/abs/2601.11581",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11581v1 Announce Type: new \nAbstract: Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, ",
    "tags": [
      "cs.CL",
      "cs.AI",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210871"
  },
  {
    "id": "010c88945105364a29b2bf728c81c167",
    "title": "Speculative Decoding: Performance or Illusion?",
    "url": "https://arxiv.org/abs/2601.11580",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11580v1 Announce Type: new \nAbstract: Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes. We present, to our knowledge, the first systematic study of SD on a production-grade and widely deployed inference engine (vLLM), covering multiple SD variants ($n$-gram, EAGLE/EAGLE-3, Draft-Model, Multi-",
    "tags": [
      "cs.CL",
      "cs.AI",
      "LLM",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210839"
  },
  {
    "id": "189bddb3f7ea3e1cc2b5441a4e297afd",
    "title": "Bielik 11B v3: Multilingual Large Language Model for European Languages",
    "url": "https://arxiv.org/abs/2601.11579",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11579v1 Announce Type: new \nAbstract: We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters via depth up-scaling. Its development involved a comprehensive four-stage training pipeline: continuous pre-training, supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforce",
    "tags": [
      "cs.CL",
      "cs.AI",
      "LLM"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210806"
  },
  {
    "id": "96ac332e8ff1f8ed19967761c780c8bb",
    "title": "LimAgents: Multi-Agent LLMs for Generating Research Limitations",
    "url": "https://arxiv.org/abs/2601.11578",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11578v1 Announce Type: new \nAbstract: Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generalizability). They usually repeat limitations reported by authors without looking at deeper methodological issues and contextual gaps. This problem is made worse because many authors disclose only partia",
    "tags": [
      "AI Agent",
      "Computer Vision",
      "cs.AI",
      "LLM",
      "cs.CL"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210773"
  },
  {
    "id": "1b6f886742485232da6074ab94c6f303",
    "title": "Concept Attractors in LLMs and their Applications",
    "url": "https://arxiv.org/abs/2601.11575",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11575v1 Announce Type: new \nAbstract: Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these Attractors to solve a ",
    "tags": [
      "cs.CL",
      "cs.AI",
      "LLM"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210735"
  },
  {
    "id": "6aebf042a23ee7b139e2d37ccc561849",
    "title": "An Empirical Analysis of Fine-Tuning Large Language Models on Bioinformatics Literature: PRSGPT and BioStarsGPT",
    "url": "https://arxiv.org/abs/2601.11573",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11573v1 Announce Type: new \nAbstract: Large language models (LLMs) often lack specialized knowledge for complex bioinformatics applications. We present a reproducible pipeline for fine-tuning LLMs on specialized bioinformatics data, demonstrated through two use cases: PRSGPT, focused on polygenic risk score (PRS) tools, and BioStarsGPT, trained on community forum discussions. The nine-step pipeline integrates diverse data sources, structured preprocessing, prompt-based question-answer",
    "tags": [
      "cs.CL",
      "GPT",
      "LLM"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210702"
  },
  {
    "id": "00f989a0290fdf378438291954cb56dc",
    "title": "Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology",
    "url": "https://arxiv.org/abs/2601.11567",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11567v1 Announce Type: new \nAbstract: Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024),",
    "tags": [
      "cs.CL",
      "cs.AI",
      "LLM",
      "GPT"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210665"
  },
  {
    "id": "7742db8df2af15b884bbcb1632548d76",
    "title": "Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths",
    "url": "https://arxiv.org/abs/2601.11564",
    "source_id": "arxiv_cl",
    "source_name": "arXiv Computation and Language",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11564v1 Announce Type: new \nAbstract: The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead. This paper investigates the critical trade-off between system performance and model quality when dense transformer architectures--specifically Llama-3.1-70B and Qwen1.5-14B--are exposed to large volu",
    "tags": [
      "cs.AI",
      "LLM",
      "cs.CL",
      "AGI",
      "Transformer"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:13.210573"
  },
  {
    "id": "dc236582b008338420ae6ab7328c0909",
    "title": "IPEC: Test-Time Incremental Prototype Enhancement Classifier for Few-Shot Learning",
    "url": "https://arxiv.org/abs/2601.11669",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11669v1 Announce Type: new \nAbstract: Metric-based few-shot approaches have gained significant popularity due to their relatively straightforward implementation, high interpret ability, and computational efficiency. However, stemming from the batch-independence assumption during testing, which prevents the model from leveraging valuable knowledge accumulated from previous batches. To address these challenges, we propose a novel test-time method called Incremental Prototype Enhancement",
    "tags": [
      "AGI",
      "cs.CV",
      "cs.LG"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.824084"
  },
  {
    "id": "ed95fba35f89c93f2ae271e51d3e5160",
    "title": "Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction",
    "url": "https://arxiv.org/abs/2601.11667",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11667v1 Announce Type: new \nAbstract: Transformer architectures deliver state-of-the-art accuracy via dense full-attention, but their quadratic time and memory complexity with respect to sequence length limits practical deployment. Linear attention mechanisms offer linear or near-linear scaling yet often incur performance degradation. Hybrid models that integrate full and linear attention layers promise a balance between efficiency and expressiveness, but face two major challenges: tr",
    "tags": [
      "cs.AI",
      "cs.LG",
      "Transformer"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.824050"
  },
  {
    "id": "78aff5b915cfa249a80eee2332b96011",
    "title": "Activation Sensitivity as a Unifying Principle for Post-Training Quantization",
    "url": "https://arxiv.org/abs/2601.11663",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11663v1 Announce Type: new \nAbstract: Post-training quantization (PTQ) methods for large language models rely on heuristics that implicitly estimate which weight channels most strongly influence model behavior. Two dominant paradigms have emerged: activation-aware methods such as AWQ prioritize channels with large activation magnitudes, while second-order methods such as GPTQ allocate quantization error according to input covariance structure. Despite strong empirical performance, the",
    "tags": [
      "GPT",
      "cs.AI",
      "LLM",
      "cs.LG"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.824016"
  },
  {
    "id": "24d68cbd16558241a5fc4e8cd997c158",
    "title": "Machine learning model for predicting surface wettability in laser-textured metal alloys",
    "url": "https://arxiv.org/abs/2601.11661",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11661v1 Announce Type: new \nAbstract: Surface wettability, governed by both topography and chemistry, plays a critical role in applications such as heat transfer, lubrication, microfluidics, and surface coatings. In this study, we present a machine learning (ML) framework capable of accurately predicting the wettability of laser-textured metal alloys using experimentally derived morphological and chemical features. Superhydrophilic and superhydrophobic surfaces were fabricated on AA60",
    "tags": [
      "cond-mat.mtrl-sci",
      "cs.LG",
      "Machine Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823980"
  },
  {
    "id": "a3e40ba52957f71f225e7bf35a12bf4b",
    "title": "Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning",
    "url": "https://arxiv.org/abs/2601.11657",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11657v1 Announce Type: new \nAbstract: Physics-aware deep learning (PADL) enables rapid prediction of complex physical systems, yet current convolutional neural network (CNN) architectures struggle with highly nonlinear flows. While scaling model size addresses complexity in broader AI, this approach yields diminishing returns for physics modeling. Drawing inspiration from Hybrid Lagrangian-Eulerian (HLE) numerical methods, we introduce deformable physics-aware recurrent convolutions (",
    "tags": [
      "cs.AI",
      "cs.LG",
      "Deep Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823944"
  },
  {
    "id": "fa87fe56529c8dfb4324ab8d5568e66c",
    "title": "Global Optimization By Gradient from Hierarchical Score-Matching Spaces",
    "url": "https://arxiv.org/abs/2601.11639",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11639v1 Announce Type: new \nAbstract: Gradient descent is the most commonly used optimization method, but limited to local optimality, and confined to the field of continuous differentiable problems with simple convex constraints. This work solve these limitations and restrictions by unifying all optimization problems with various complex constraints as a general hierarchical optimization objective without constraints, which is optimized by gradient obtained through score matching. By",
    "tags": [
      "cs.LG"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823910"
  },
  {
    "id": "c23bdca9820d310f44125079ba361262",
    "title": "Verifying Physics-Informed Neural Network Fidelity using Classical Fisher Information from Differentiable Dynamical System",
    "url": "https://arxiv.org/abs/2601.11638",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11638v1 Announce Type: new \nAbstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations and modeling physical systems by embedding physical laws into the learning process. However, rigorously quantifying how well a PINN captures the complete dynamical behavior of the system, beyond simple trajectory prediction, remains a challenge. This paper proposes a novel experimental framework to address this by employing Fisher informatio",
    "tags": [
      "cs.LG",
      "stat.ML"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823875"
  },
  {
    "id": "382b8f5ddabcd8c8ed13c2ddf7f9af6b",
    "title": "NoiseFormer -- Noise Diffused Symmetric Attention Transformer",
    "url": "https://arxiv.org/abs/2601.11619",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11619v1 Announce Type: new \nAbstract: Transformer architecture has been very successful long runner in the field of Deep Learning (DL) and Large Language Models (LLM) because of its powerful attention-based learning and parallel-natured architecture. As the models grow gigantic in terms of memory footprint, difficulties in fitting the model on a device like a GPU or an AI accelerator give rise to the need for multiple computing devices thereby escalating the computing cost. This incre",
    "tags": [
      "cs.LG",
      "Deep Learning",
      "cs.AI",
      "LLM",
      "Transformer"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823838"
  },
  {
    "id": "be659aad908495417b302e2cc7cec3ab",
    "title": "Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention",
    "url": "https://arxiv.org/abs/2601.11618",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11618v1 Announce Type: new \nAbstract: Geometric Attention (GA) specifies an attention layer by four independent inputs: a finite carrier (what indices are addressable), an evidence-kernel rule (how masked proto-scores and a link induce nonnegative weights), a probe family (which observables are treated as admissible), and an anchor/update rule (which representative kernel is selected and how it is applied). Probe families induce an operational equivalence relation on kernels and there",
    "tags": [
      "cs.AI",
      "cs.LG",
      "Transformer"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823802"
  },
  {
    "id": "513a6f5721e256519c6adbc3ccef1c44",
    "title": "Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective",
    "url": "https://arxiv.org/abs/2601.11616",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11616v1 Announce Type: new \nAbstract: Mixture-of-Experts (MoE) architectures are commonly motivated by efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly characterized. In this work, we study MoEs through a geometric lens, interpreting routing as a form of soft partitioning of the representation space into overlapping local charts. We introduce a Dual Jacobian-PCA Spectral Geometry probe. It analyzes local f",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823764"
  },
  {
    "id": "727b384b1439f39210d64bdb93e20cfd",
    "title": "A Review on Machine Learning Approaches for the Prediction of Glucose Levels and Hypogylcemia",
    "url": "https://arxiv.org/abs/2601.11615",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11615v1 Announce Type: new \nAbstract: Type 1 Diabetes (T1D) is an autoimmune disease leading to insulin insufficiency. Thus, patients require lifelong insulin therapy, which has a side effect of hypoglycemia. Hypoglycemia is a critical state of decreased blood glucose levels (BGL) below 70 mg/dL and is associated with increased risk of mortality. Machine learning (ML) models can improve diabetes management by predicting hypoglycemia and providing optimal prevention methods. ML models ",
    "tags": [
      "cs.LG",
      "Machine Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823714"
  },
  {
    "id": "aa882610af5a81f140bfa6234774c107",
    "title": "Integrating Temporal Context into Streaming Data for Human Activity Recognition in Smart Home",
    "url": "https://arxiv.org/abs/2601.11611",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11611v1 Announce Type: new \nAbstract: With the global population ageing, it is crucial to enable individuals to live independently and safely in their homes. Using ubiquitous sensors such as Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest for monitoring daily activities and facilitating preventative healthcare interventions for the elderly. Human Activity Recognition (HAR) from passive sensors mostly relies on traditional machine learning and includes da",
    "tags": [
      "cs.LG",
      "Reinforcement Learning",
      "Machine Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823554"
  },
  {
    "id": "9ad71e72f9c27b5804d1a7629eb4e5e4",
    "title": "Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory Storage Model Based on Invertible Compression and Learnable Prediction",
    "url": "https://arxiv.org/abs/2601.11609",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11609v1 Announce Type: new \nAbstract: Current large language models (LLMs) generally lack an effective runtime memory mechanism,making it difficult to adapt to dynamic and personalized interaction requirements. To address this issue, this paper proposes a novel neural memory storage architecture--the Auxiliary Prediction Compression Memory Model (ApCM Model).",
    "tags": [
      "LLM",
      "cs.LG"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823486"
  },
  {
    "id": "a5b855af42ee29e6d6ca0fdce88744f3",
    "title": "A Multimodal Data Processing Pipeline for MIMIC-IV Dataset",
    "url": "https://arxiv.org/abs/2601.11606",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11606v1 Announce Type: new \nAbstract: The MIMIC-IV dataset is a large, publicly available electronic health record (EHR) resource widely used for clinical machine learning research. It comprises multiple modalities, including structured data, clinical notes, waveforms, and imaging data. Working with these disjointed modalities requires an extensive manual effort to preprocess and align them for downstream analysis. While several pipelines for MIMIC-IV data extraction are available, th",
    "tags": [
      "AGI",
      "cs.LG",
      "Machine Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823448"
  },
  {
    "id": "ebce10af37b06af2316eabdb254df999",
    "title": "GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment",
    "url": "https://arxiv.org/abs/2601.11574",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11574v1 Announce Type: new \nAbstract: Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gra",
    "tags": [
      "cs.AI",
      "LLM",
      "cs.LG",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823277"
  },
  {
    "id": "ccd20047c7926a5b1876458626506777",
    "title": "Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces",
    "url": "https://arxiv.org/abs/2601.11572",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11572v1 Announce Type: new \nAbstract: We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the observation that LLM embeddings exhibit distinct states, suggesting discrete semantic representations, we explore the application of these mathematical tools to analyze semantic relationships. We demons",
    "tags": [
      "cs.AI",
      "LLM",
      "cs.LG",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823199"
  },
  {
    "id": "7f755d966a1d8afd518172bba4ba0a28",
    "title": "AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control",
    "url": "https://arxiv.org/abs/2601.11568",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11568v1 Announce Type: new \nAbstract: Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($\\rho$) and update frequency ($T$) -- require costly manual tuning, limiting adaptability. We present AdaFRUGAL, which automates this process by introducing two dynamic controls: (i) a linear decay for $\\rho$ to progressively reduce memor",
    "tags": [
      "cs.CL",
      "cs.AI",
      "LLM",
      "cs.LG"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823152"
  },
  {
    "id": "9c00cb6c6242a76088c88ac9b5068a77",
    "title": "CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration",
    "url": "https://arxiv.org/abs/2601.11556",
    "source_id": "arxiv_lg",
    "source_name": "arXiv Machine Learning",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11556v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item in",
    "tags": [
      "cs.LG",
      "cs.SD",
      "eess.AS",
      "cs.AI",
      "LLM",
      "cs.CL"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:10.823108"
  },
  {
    "id": "b4eb67f972e15cc25064c7ec48f0b20c",
    "title": "Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability Trade-offs in Novel Structured Output Formats",
    "url": "https://arxiv.org/abs/2601.12014",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.12014v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. In this paper, we argue that structured output formats should be assessed not only in terms of correctness, but also with respect to",
    "tags": [
      "cs.AI",
      "LLM",
      "Reinforcement Learning",
      "cs.SE"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922774"
  },
  {
    "id": "2ca7d9035d0aac6d7769c336788f3d27",
    "title": "Kernel-Based Learning of Safety Barriers",
    "url": "https://arxiv.org/abs/2601.12002",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.12002v1 Announce Type: new \nAbstract: The rapid integration of AI algorithms in safety-critical applications such as autonomous driving and healthcare is raising significant concerns about the ability to meet stringent safety standards. Traditional tools for formal safety verification struggle with the black-box nature of AI-driven systems and lack the flexibility needed to scale to the complexity of real-world applications. In this paper, we present a data-driven approach for safety ",
    "tags": [
      "eess.SY",
      "cs.LG",
      "Reinforcement Learning",
      "AI Agent",
      "cs.AI",
      "cs.SY"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922737"
  },
  {
    "id": "6eea1b7bbfe54dbd5c29647de5500e6a",
    "title": "Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion",
    "url": "https://arxiv.org/abs/2601.11979",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11979v1 Announce Type: new \nAbstract: In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often a",
    "tags": [
      "cs.AI",
      "LLM",
      "cs.LG"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922698"
  },
  {
    "id": "f30e8e62f57a3a0127d523de5e784911",
    "title": "Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement",
    "url": "https://arxiv.org/abs/2601.11974",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11974v1 Announce Type: new \nAbstract: While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-e",
    "tags": [
      "cs.AI",
      "LLM",
      "AI Agent"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922664"
  },
  {
    "id": "0e44316e194915fc8dffde947a1640ae",
    "title": "Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart",
    "url": "https://arxiv.org/abs/2601.11940",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11940v1 Announce Type: new \nAbstract: Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root e",
    "tags": [
      "cs.CL",
      "cs.AI",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922634"
  },
  {
    "id": "4cef4110bd8b1e198c195e0fe997e920",
    "title": "LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning",
    "url": "https://arxiv.org/abs/2601.11905",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11905v1 Announce Type: new \nAbstract: We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalize",
    "tags": [
      "stat.TH",
      "cs.LG",
      "Machine Learning",
      "cs.AI",
      "LLM",
      "math.ST"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922601"
  },
  {
    "id": "08bf10ff49569a6b0e3af30768713d67",
    "title": "AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems",
    "url": "https://arxiv.org/abs/2601.11903",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11903v1 Announce Type: new \nAbstract: Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present ",
    "tags": [
      "cs.AI",
      "LLM",
      "AI Agent"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922560"
  },
  {
    "id": "63191d8342a5e1c6922e9cd828fc5c14",
    "title": "MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment",
    "url": "https://arxiv.org/abs/2601.11885",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11885v1 Announce Type: new \nAbstract: Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph tr",
    "tags": [
      "Computer Vision",
      "cs.AI",
      "Transformer",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922529"
  },
  {
    "id": "93bb10d0675ec86ed3c35a815bff6176",
    "title": "Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority",
    "url": "https://arxiv.org/abs/2601.11850",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11850v1 Announce Type: new \nAbstract: The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles.",
    "tags": [
      "cs.AI",
      "GPT",
      "cs.CY"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922498"
  },
  {
    "id": "d7cd3473a1a3644c8f6a05aadccc5474",
    "title": "Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic",
    "url": "https://arxiv.org/abs/2601.11840",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11840v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.\n  We present CodeLogician, a neurosymbolic agent for precise analysis o",
    "tags": [
      "Reinforcement Learning",
      "AI Agent",
      "cs.LO",
      "cs.AI",
      "LLM",
      "cs.SE"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922463"
  },
  {
    "id": "e9ac0ec7c2853e3dcb5a86b53b3809e6",
    "title": "AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept",
    "url": "https://arxiv.org/abs/2601.11825",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11825v1 Announce Type: new \nAbstract: Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge g",
    "tags": [
      "cs.AI",
      "cs.IR"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922427"
  },
  {
    "id": "fc5c05d1f5b36ba5589cb48fdbe08f7b",
    "title": "POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation",
    "url": "https://arxiv.org/abs/2601.11816",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11816v1 Announce Type: new \nAbstract: Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked di",
    "tags": [
      "cs.AI",
      "LLM",
      "AI Agent"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922389"
  },
  {
    "id": "e743445523aa605c283aae62de6623d4",
    "title": "A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation",
    "url": "https://arxiv.org/abs/2601.11792",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11792v1 Announce Type: new \nAbstract: Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (",
    "tags": [
      "cs.CL",
      "cs.AI",
      "LLM"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922322"
  },
  {
    "id": "455181511816eda19a4af636fca4efc0",
    "title": "Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles",
    "url": "https://arxiv.org/abs/2601.11781",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11781v1 Announce Type: new \nAbstract: Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) ",
    "tags": [
      "cs.AI",
      "cs.CV",
      "AI Agent"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922284"
  },
  {
    "id": "155174921bf985d2f66c14a07d12b291",
    "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement",
    "url": "https://arxiv.org/abs/2601.11747",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11747v1 Announce Type: new \nAbstract: Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas desi",
    "tags": [
      "cs.AI"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922246"
  },
  {
    "id": "7daccc18d9e69a683e8434453764f5e6",
    "title": "Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance",
    "url": "https://arxiv.org/abs/2601.11625",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11625v1 Announce Type: new \nAbstract: Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consist",
    "tags": [
      "cs.AI",
      "cs.LG",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922209"
  },
  {
    "id": "3209f2324d4529d05ffb47a59150e13b",
    "title": "Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models",
    "url": "https://arxiv.org/abs/2601.11622",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11622v1 Announce Type: new \nAbstract: Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transforme",
    "tags": [
      "cs.AI",
      "LLM",
      "cs.LG",
      "Reinforcement Learning"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922174"
  },
  {
    "id": "de26442976bb2c0ff23b61cdcecbcb87",
    "title": "A Mind Cannot Be Smeared Across Time",
    "url": "https://arxiv.org/abs/2601.11620",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11620v1 Announce Type: new \nAbstract: Whether machines can be conscious depends not only on what they compute, but \\emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal s",
    "tags": [
      "cs.AI"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922136"
  },
  {
    "id": "35a5ddd96c66fbb201763af18f3476f9",
    "title": "MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?",
    "url": "https://arxiv.org/abs/2601.11559",
    "source_id": "arxiv_ai",
    "source_name": "arXiv AI",
    "language": "en",
    "category": "research",
    "summary": "arXiv:2601.11559v1 Announce Type: new \nAbstract: Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD code",
    "tags": [
      "cs.LG",
      "Reinforcement Learning",
      "cs.AI",
      "LLM",
      "cs.CL"
    ],
    "publish_time": "2026-01-21T05:00:00",
    "fetch_time": "2026-01-22T09:14:07.922089"
  },
  {
    "id": "2f3dd20f59dd91ea58c5cef26a7f3d20",
    "title": "Going beyond pilots with composable and sovereign AI",
    "url": "https://www.technologyreview.com/2026/01/19/1131422/going-beyond-pilots-with-composable-and-sovereign-ai/",
    "source_id": "mit_tech_review",
    "source_name": "MIT Technology Review AI",
    "language": "en",
    "category": "news",
    "summary": "Today marks an inflection point for enterprise AI adoption. Despite billions invested in generative AI, only 5% of integrated pilots deliver measurable business value and nearly one in two companies abandons AI initiatives before reaching production. The bottleneck is not the models themselves. Whatâ€™s holding enterprises back is the surrounding infrastructure: Limited data accessibility, rigidâ€¦",
    "tags": [
      "Artificial intelligence",
      "Reinforcement Learning",
      "sponsored"
    ],
    "publish_time": "2026-01-19T11:59:33",
    "fetch_time": "2026-01-22T09:14:04.349798"
  },
  {
    "id": "6edf5159be39a2d5077cc5f0fdebef4f",
    "title": "The UK government is backing AI that can run its own lab experiments",
    "url": "https://www.technologyreview.com/2026/01/20/1131462/the-uk-government-is-backing-ai-scientists-that-can-run-their-own-experiments/",
    "source_id": "mit_tech_review",
    "source_name": "MIT Technology Review AI",
    "language": "en",
    "category": "news",
    "summary": "A number of startups and university teams that are building â€œAI scientistsâ€ to design and run experiments in the lab, including robot biologists and chemists, have just won extra funding from the UK government agency that supports moonshot R&D. The competition, set up by ARIA (the Advanced Research and Invention Agency), gives a clear senseâ€¦",
    "tags": [
      "Artificial intelligence",
      "Robotics",
      "App"
    ],
    "publish_time": "2026-01-20T13:28:21",
    "fetch_time": "2026-01-22T09:14:04.349759"
  },
  {
    "id": "3dcfd0c99efd35cf2c48696162ad23d8",
    "title": "The era of agentic chaos and how data will save us",
    "url": "https://www.technologyreview.com/2026/01/20/1130911/the-era-of-agentic-chaos-and-how-data-will-save-us/",
    "source_id": "mit_tech_review",
    "source_name": "MIT Technology Review AI",
    "language": "en",
    "category": "news",
    "summary": "AI agents are moving beyond coding assistants and customer service chatbots into the operational core of the enterprise. The ROI is promising, but autonomy without alignment is a recipe for chaos. Business leaders need to lay the essential foundations now. The agent explosion is coming Agents are independently handling end-to-end processes across lead generation, supplyâ€¦",
    "tags": [
      "Artificial intelligence",
      "AI Agent",
      "sponsored"
    ],
    "publish_time": "2026-01-20T15:00:00",
    "fetch_time": "2026-01-22T09:14:04.349712"
  },
  {
    "id": "53096fd4d5a5890a81542b0af56e7419",
    "title": "Everyone wants AI sovereignty. No one can truly have it.",
    "url": "https://www.technologyreview.com/2026/01/21/1131513/everyone-wants-ai-sovereignty-no-one-can-truly-have-it/",
    "source_id": "mit_tech_review",
    "source_name": "MIT Technology Review AI",
    "language": "en",
    "category": "news",
    "summary": "Governments plan to pour $1.3 trillion into AI infrastructure by 2030 to invest in â€œsovereign AI,â€ with the premise being that countries should be in control of their own AI capabilities. The funds include financing for domestic data centers, locally trained models, independent supply chains, and national talent pipelines. This is a response to realâ€¦",
    "tags": [
      "Artificial intelligence",
      "AI",
      "App",
      "artificial intelligence",
      "Opinion"
    ],
    "publish_time": "2026-01-21T14:00:00",
    "fetch_time": "2026-01-22T09:14:04.349665"
  },
  {
    "id": "fac6d44a5a89e3d2a2a2669352b4e9e7",
    "title": "Rethinking AIâ€™s future in an augmented workplace",
    "url": "https://www.technologyreview.com/2026/01/21/1131366/rethinking-ais-future-in-an-augmented-workplace/",
    "source_id": "mit_tech_review",
    "source_name": "MIT Technology Review AI",
    "language": "en",
    "category": "news",
    "summary": "There are many paths AI evolution could take. On one end of the spectrum, AI is dismissed as a marginal fad, another bubble fueled by notoriety and misallocated capital. On the other end, itâ€™s cast as a dystopian force, destined to eliminate jobs on a large scale and destabilize economies. Markets oscillate between skepticism andâ€¦",
    "tags": [
      "Artificial intelligence",
      "sponsored"
    ],
    "publish_time": "2026-01-21T15:00:00",
    "fetch_time": "2026-01-22T09:14:04.349608"
  },
  {
    "id": "c7337847f58c10738e85a9ead4325be0",
    "title": "Indian vibe-coding startup Emergent triples valuation to $300M with $70M fundraise",
    "url": "https://techcrunch.com/2026/01/20/indian-vibe-coding-startup-emergent-raises-70m-at-300m-valuation-from-softbank-khosla-ventures/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "The funding comes as the startup claims it has scaled ARR to $50 million and is targeting $100 million by April 2026.",
    "tags": [
      "AI",
      "Softbank",
      "vibe coding",
      "emergent",
      "Khosla Ventures",
      "lightspeed venture partners",
      "Startups"
    ],
    "publish_time": "2026-01-20T13:50:09",
    "fetch_time": "2026-01-22T09:14:01.493900"
  },
  {
    "id": "eac35d858ea29cde0678af871563399b",
    "title": "Humans&, a â€˜human-centricâ€™ AI startup founded by Anthropic, xAI, Google alums, raised $480M seed round",
    "url": "https://techcrunch.com/2026/01/20/humans-a-human-centric-ai-startup-founded-by-anthropic-xai-google-alums-raised-480m-seed-round/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "Humans&, a startup that believes AI should empower people, not replace them, has reportedly raised a $480 million seed round at a $4.48 billion valuation.",
    "tags": [
      "humans&",
      "AI",
      "In Brief",
      "Google",
      "Anthropic",
      "Fundraising",
      "xAI",
      "Startups"
    ],
    "publish_time": "2026-01-20T16:00:57",
    "fetch_time": "2026-01-22T09:14:01.493869"
  },
  {
    "id": "d9852b9e9efd2b8689e1c20bbf1a59ea",
    "title": "Elon Musk says Teslaâ€™s restarted Dojo3 will be for â€˜space-based AI computeâ€™",
    "url": "https://techcrunch.com/2026/01/20/elon-musk-says-teslas-restarted-dojo3-will-be-for-space-based-ai-compute/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "Tesla aims to restart work on Dojo3, its previously abandoned third-generation AI chip. Only this time, Dojo3 wonâ€™t be aimed at training self-driving models on Earth. Instead, Musk says it will be dedicated to â€œspace-based AI compute.â€",
    "tags": [
      "AI",
      "Transportation",
      "Elon Musk",
      "SpaceX",
      "Tesla",
      "Space",
      "Dojo"
    ],
    "publish_time": "2026-01-20T22:10:41",
    "fetch_time": "2026-01-22T09:14:01.493824"
  },
  {
    "id": "e6873109f887113f8d14b1ca655bae9c",
    "title": "In an effort to protect young users, ChatGPT will now predict how old you are",
    "url": "https://techcrunch.com/2026/01/20/in-an-effort-to-protect-young-users-chatgpt-will-now-predict-how-old-you-are/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "The feature is designed to stop problematic content from being delivered to users under the age of 18.",
    "tags": [
      "OpenAI",
      "AI",
      "In Brief",
      "artificial intelligence",
      "ChatGPT",
      "GPT",
      "persona"
    ],
    "publish_time": "2026-01-20T23:29:56",
    "fetch_time": "2026-01-22T09:14:01.493790"
  },
  {
    "id": "4b1549c35c3d074627654d47959c28a1",
    "title": "Bolna nabs $6.3M from General Catalyst for its India-focused voice orchestration platform",
    "url": "https://techcrunch.com/2026/01/20/bolna-nabs-6-3-million-from-general-catalyst-for-its-india-focused-voice-orchestration-platform/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "Bolna said that 75% of its revenue is coming from self-serve customers.",
    "tags": [
      "blume ventures",
      "AI",
      "General Catalyst",
      "voice AI",
      "Fundraising",
      "India"
    ],
    "publish_time": "2026-01-21T02:00:00",
    "fetch_time": "2026-01-22T09:14:01.493729"
  },
  {
    "id": "024b456c0885520740225b031231168c",
    "title": "Consumers spent more on mobile apps than games in 2025, driven by AI app adoption",
    "url": "https://techcrunch.com/2026/01/21/consumers-spent-more-on-mobile-apps-than-games-in-2025-driven-by-ai-app-adoption/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "Consumers spent more money in mobile apps than games in 2025, driven by AI app adoption.",
    "tags": [
      "AI",
      "sensor tower",
      "trends",
      "ai apps",
      "Commerce",
      "mobile apps",
      "Apps"
    ],
    "publish_time": "2026-01-21T11:30:00",
    "fetch_time": "2026-01-22T09:14:01.493699"
  },
  {
    "id": "975cedcfaf0e6f46db4a5a8703fc802e",
    "title": "Language learning marketplace Preplyâ€™s unicorn status embodies Ukrainian resilience",
    "url": "https://techcrunch.com/2026/01/21/language-learning-marketplace-preplys-unicorn-status-embodies-ukrainian-resilience/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "Language learning marketplace Preply is now valued at $1.2 billion after raising a $150 million Series D round that marks a new chapter for the 14-year-old company.",
    "tags": [
      "AI",
      "Preply",
      "Fundraising",
      "EdTech",
      "language learning",
      "Westcap",
      "Startups"
    ],
    "publish_time": "2026-01-21T12:00:00",
    "fetch_time": "2026-01-22T09:14:01.493668"
  },
  {
    "id": "16bdb5b037edae07c97a2573fbeb175e",
    "title": "Zanskar thinks 1 TW of geothermal power is being overlooked",
    "url": "https://techcrunch.com/2026/01/21/zanskar-thinks-1-tw-of-geothermal-power-is-being-overlooked/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "Zanskar has raised $115 million to find about a dozen geothermal resources that could help power the grid throughout the U.S. West.",
    "tags": [
      "AI",
      "Climate",
      "Reinforcement Learning",
      "Fundraising",
      "geothermal energy",
      "geothermal",
      "physical ai"
    ],
    "publish_time": "2026-01-21T13:00:00",
    "fetch_time": "2026-01-22T09:14:01.493636"
  },
  {
    "id": "2ae675e4cfe3021fb38b0b2e771a5368",
    "title": "Adobe Acrobat now lets you edit files using prompts, generate podcast summaries",
    "url": "https://techcrunch.com/2026/01/21/adobe-acrobat-now-lets-you-edit-files-using-prompts-generate-podcast-summaries/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "Adobe is adding AI tools to Acrobat, including the ability to generate podcast summaries of files, create presentations, and a way for users to edit files using prompts.",
    "tags": [
      "Adobe",
      "AI",
      "PDF",
      "Adobe Acrobat",
      "Apps"
    ],
    "publish_time": "2026-01-21T14:16:53",
    "fetch_time": "2026-01-22T09:14:01.493605"
  },
  {
    "id": "25b508863fb6cf34b6f8a1443eb8828c",
    "title": "OpenAI aims to ship its first device in 2026, and it could be earbuds",
    "url": "https://techcrunch.com/2026/01/21/openai-aims-to-ship-its-first-device-in-2026-and-it-could-be-earbuds/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "The AI startup is on track to announce its first hardware device in the second half of this year, OpenAI Chief Global Affairs Officer Chris Lehane said during an interview at Davos.",
    "tags": [
      "OpenAI",
      "AI",
      "Hardware",
      "TC",
      "jony ive",
      "davos"
    ],
    "publish_time": "2026-01-21T15:20:23",
    "fetch_time": "2026-01-22T09:14:01.493538"
  },
  {
    "id": "c272dfee5aab2db24f5a071b43019d57",
    "title": "YouTube will soon let creators make Shorts with their own AI likeness",
    "url": "https://techcrunch.com/2026/01/21/youtube-will-soon-let-creators-make-shorts-with-their-own-ai-likeness/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "YouTube Shorts viewers might soon see AI versions of their favorite creators when scrolling through their feeds.",
    "tags": [
      "AI",
      "Media & Entertainment",
      "creator economy",
      "YouTube Shorts",
      "YouTube",
      "Apps"
    ],
    "publish_time": "2026-01-21T15:41:15",
    "fetch_time": "2026-01-22T09:14:01.493506"
  },
  {
    "id": "da68660605af63d93980ea42fca45853",
    "title": "Irony alert: Hallucinated citations found in papers from NeurIPS, the prestigious AI conference",
    "url": "https://techcrunch.com/2026/01/21/irony-alert-hallucinated-citations-found-in-papers-from-neurips-the-prestigious-ai-conference/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "Research from startup GPTZero points to the impossible problem prestigious conferences face in the age of AI slop.",
    "tags": [
      "AI",
      "GPT",
      "GPTZero",
      "hallucinations",
      "Startups"
    ],
    "publish_time": "2026-01-21T20:34:42",
    "fetch_time": "2026-01-22T09:14:01.493475"
  },
  {
    "id": "b1a5aba5e3b6f2dc03de7d7264a75eb5",
    "title": "Apple plans to make Siri an AI chatbot, report says",
    "url": "https://techcrunch.com/2026/01/21/apple-plans-to-make-siri-an-ai-chatbot-report-says/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "Siri could look more like ChatGPT than its current state as an integrated feature across Apple products.",
    "tags": [
      "Apple",
      "AI",
      "In Brief",
      "GPT",
      "siri"
    ],
    "publish_time": "2026-01-21T22:12:50",
    "fetch_time": "2026-01-22T09:14:01.493415"
  },
  {
    "id": "bc76c5f6328af20d789065a647c5f9c2",
    "title": "Todoistâ€™s app now lets you add tasks to your to-do list by speaking to its AI",
    "url": "https://techcrunch.com/2026/01/21/todoists-app-now-lets-you-add-tasks-to-your-to-do-list-by-speaking-to-its-ai/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "The feature, now public, lets you create to-do's and action items by speaking naturally to the app's AI.",
    "tags": [
      "todoist",
      "productivity",
      "AI",
      "Apps"
    ],
    "publish_time": "2026-01-21T22:19:17",
    "fetch_time": "2026-01-22T09:14:01.493385"
  },
  {
    "id": "84f08a3e0b63512a7b3ee546072196d4",
    "title": "Sources: Project SGLang spins out as RadixArk with $400M valuation as inference market explodes",
    "url": "https://techcrunch.com/2026/01/21/sources-project-sglang-spins-out-as-radixark-with-400m-valuation-as-inference-market-explodes/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "SGLang, which originated as an open source research project at Ion Stoicaâ€™s UC Berkeley lab, has raised capital from Accel.",
    "tags": [
      "AI",
      "inference optimization",
      "Exclusive",
      "Venture",
      "infrastructure software",
      "Accel"
    ],
    "publish_time": "2026-01-21T23:24:14",
    "fetch_time": "2026-01-22T09:14:01.493311"
  },
  {
    "id": "9a0cb8095fd8fb9a0c2b33416b3e7503",
    "title": "Not to be outdone by OpenAI, Apple is reportedly developing an AI wearable",
    "url": "https://techcrunch.com/2026/01/21/not-to-be-outdone-by-openai-apple-is-reportedly-developing-an-ai-wearable/",
    "source_id": "techcrunch_ai",
    "source_name": "TechCrunch AI",
    "language": "en",
    "category": "news",
    "summary": "Should this wearable materialize, it could be released as early as 2027, according to a report on the device.",
    "tags": [
      "OpenAI",
      "Apple",
      "AI",
      "Hardware",
      "Reinforcement Learning",
      "Ai Pin",
      "In Brief",
      "AI wearable"
    ],
    "publish_time": "2026-01-22T00:20:18",
    "fetch_time": "2026-01-22T09:14:01.493260"
  },
  {
    "id": "6f5e1c992569ccd84c01406c068e3db9",
    "title": "AprielGuard: A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems",
    "url": "https://huggingface.co/blog/ServiceNow-AI/aprielguard",
    "source_id": "huggingface",
    "source_name": "Hugging Face Blog",
    "language": "en",
    "category": "research",
    "summary": "",
    "tags": [
      "LLM"
    ],
    "publish_time": "2025-12-23T14:07:35",
    "fetch_time": "2026-01-22T09:13:16.770204"
  },
  {
    "id": "0cf2d67bbdbe444667aad3f8c918fef1",
    "title": "Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture",
    "url": "https://huggingface.co/blog/tiiuae/falcon-h1-arabic",
    "source_id": "huggingface",
    "source_name": "Hugging Face Blog",
    "language": "en",
    "category": "research",
    "summary": "",
    "tags": [],
    "publish_time": "2026-01-05T09:16:51",
    "fetch_time": "2026-01-22T09:13:16.770194"
  },
  {
    "id": "33da4e6187689a06eacf2608426dab75",
    "title": "NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI",
    "url": "https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning",
    "source_id": "huggingface",
    "source_name": "Hugging Face Blog",
    "language": "en",
    "category": "research",
    "summary": "",
    "tags": [],
    "publish_time": "2026-01-05T22:56:51",
    "fetch_time": "2026-01-22T09:13:16.770188"
  },
  {
    "id": "819652200c9ae47e310445a53b91da3d",
    "title": "Differential Transformer V2",
    "url": "https://huggingface.co/blog/microsoft/diff-attn-v2",
    "source_id": "huggingface",
    "source_name": "Hugging Face Blog",
    "language": "en",
    "category": "research",
    "summary": "",
    "tags": [
      "Transformer"
    ],
    "publish_time": "2026-01-20T03:20:57",
    "fetch_time": "2026-01-22T09:13:16.770172"
  },
  {
    "id": "7883b0bfc9f9693d9e2f07d68a18af32",
    "title": "AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality",
    "url": "https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face",
    "source_id": "huggingface",
    "source_name": "Hugging Face Blog",
    "language": "en",
    "category": "research",
    "summary": "",
    "tags": [
      "AI Agent"
    ],
    "publish_time": "2026-01-21T06:25:31",
    "fetch_time": "2026-01-22T09:13:16.770164"
  }
]