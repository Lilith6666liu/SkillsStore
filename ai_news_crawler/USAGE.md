# AIèµ„è®¯æŠ“å–ç³»ç»Ÿ - ä½¿ç”¨æŒ‡å—

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### æ–¹å¼1: ä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
cd ai_news_crawler
./start.sh
```

ç„¶åæ ¹æ®èœå•é€‰æ‹©æ“ä½œå³å¯ã€‚

### æ–¹å¼2: ç›´æ¥ä½¿ç”¨å‘½ä»¤

```bash
# 1. è¿›å…¥é¡¹ç›®ç›®å½•
cd ai_news_crawler

# 2. å®‰è£…ä¾èµ–ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰
pip3 install -r requirements.txt

# 3. æŠ“å–èµ„è®¯
python3 main.py
```

## ğŸ“‹ å¸¸ç”¨å‘½ä»¤

### åŸºç¡€æŠ“å–

```bash
# æŠ“å–æ‰€æœ‰æºçš„æœ€æ–°èµ„è®¯
python3 main.py

# åªæŠ“å–æœ€è¿‘24å°æ—¶çš„å†…å®¹
python3 main.py --hours 24

# åªæŠ“å–æŒ‡å®šçš„æ•°æ®æº
python3 main.py --sources openai huggingface techcrunch_ai

# å¯¼å‡ºä¸ºCSVæ ¼å¼
python3 main.py --output csv --file ai_news.csv
```

### å®šæ—¶è‡ªåŠ¨æŠ“å–

```bash
# æ¯å°æ—¶è‡ªåŠ¨æŠ“å–ä¸€æ¬¡
python3 scheduler.py --interval 1h

# æ¯30åˆ†é’ŸæŠ“å–ä¸€æ¬¡
python3 scheduler.py --interval 30m

# æ¯å¤©æ—©ä¸Š9ç‚¹æŠ“å–
python3 scheduler.py --cron "0 9 * * *"

# åªæ‰§è¡Œä¸€æ¬¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
python3 scheduler.py --once
```

### Webç•Œé¢æŸ¥çœ‹

```bash
# å¯åŠ¨WebæœåŠ¡å™¨
python3 web_viewer.py

# ç„¶ååœ¨æµè§ˆå™¨è®¿é—®: http://127.0.0.1:5000
```

## âš™ï¸ é…ç½®è¯´æ˜

ç¼–è¾‘ `config.yaml` æ–‡ä»¶å¯ä»¥è‡ªå®šä¹‰é…ç½®ï¼š

### æ•°æ®æºé…ç½®

```yaml
sources:
  update_interval: 3600  # æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
  max_articles_per_source: 20  # æ¯ä¸ªæºæœ€å¤šæŠ“å–æ–‡ç« æ•°
```

### è¿‡æ»¤é…ç½®

```yaml
filter:
  deduplication: true  # æ˜¯å¦å»é‡
  keywords:  # å…³é”®è¯è¿‡æ»¤ï¼ˆåŒ…å«è¿™äº›è¯çš„æ–‡ç« ä¼šè¢«ä¿ç•™ï¼‰
    - AI
    - äººå·¥æ™ºèƒ½
    - machine learning
    - GPT
  time_range_hours: 0  # æ—¶é—´èŒƒå›´ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼‰
```

### å­˜å‚¨é…ç½®

```yaml
storage:
  type: json  # å­˜å‚¨ç±»å‹: json, sqlite, csv
  path: ./data/ai_news.json  # JSONæ–‡ä»¶è·¯å¾„
```

## ğŸ“Š æ•°æ®æºåˆ—è¡¨

### å›½é™…æºï¼ˆ10ä¸ªï¼‰
- OpenAI Blog
- Google AI Blog
- Hugging Face Blog
- TechCrunch AI
- VentureBeat AI
- MIT Technology Review
- arXiv (AI/ML/CL)
- AWS Machine Learning Blog
- DeepMind Blog

### å›½å†…æºï¼ˆ5ä¸ªï¼‰
- æœºå™¨ä¹‹å¿ƒ
- é‡å­ä½
- é›·é”‹ç½‘AI
- 36æ°ªAI
- AIç§‘æŠ€è¯„è®º

## ğŸ” æŸ¥çœ‹æ•°æ®

### æ–¹å¼1: Webç•Œé¢ï¼ˆæ¨èï¼‰

```bash
python3 web_viewer.py
```

è®¿é—® http://127.0.0.1:5000 æŸ¥çœ‹ç²¾ç¾çš„Webç•Œé¢

### æ–¹å¼2: ç›´æ¥æŸ¥çœ‹JSONæ–‡ä»¶

```bash
cat data/ai_news.json
```

### æ–¹å¼3: ä½¿ç”¨jqå·¥å…·ï¼ˆéœ€å®‰è£…jqï¼‰

```bash
# æŸ¥çœ‹æœ€æ–°10ç¯‡æ–‡ç« æ ‡é¢˜
cat data/ai_news.json | jq -r '.[:10] | .[] | .title'

# æŒ‰åˆ†ç±»ç»Ÿè®¡
cat data/ai_news.json | jq 'group_by(.category) | map({category: .[0].category, count: length})'

# æŸ¥çœ‹ä¸­æ–‡æ–‡ç« 
cat data/ai_news.json | jq '.[] | select(.language=="zh")'
```

## ğŸ¤– è‡ªåŠ¨åŒ–è¿è¡Œ

### ä½¿ç”¨cronå®šæ—¶ä»»åŠ¡ï¼ˆMac/Linuxï¼‰

```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ ä»¥ä¸‹è¡Œï¼ˆæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰
0 * * * * cd /path/to/ai_news_crawler && python3 main.py >> logs/cron.log 2>&1
```

### ä½¿ç”¨launchdï¼ˆMacæ¨èï¼‰

åˆ›å»º `~/Library/LaunchAgents/com.ai.news.crawler.plist`:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.ai.news.crawler</string>
    <key>ProgramArguments</key>
    <array>
        <string>/usr/bin/python3</string>
        <string>/path/to/ai_news_crawler/main.py</string>
    </array>
    <key>StartInterval</key>
    <integer>3600</integer>
    <key>RunAtLoad</key>
    <true/>
</dict>
</plist>
```

åŠ è½½ä»»åŠ¡ï¼š
```bash
launchctl load ~/Library/LaunchAgents/com.ai.news.crawler.plist
```

## ğŸ¨ è¾“å‡ºæ ¼å¼

æ¯ç¯‡æ–‡ç« åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{
  "id": "å”¯ä¸€ID",
  "title": "æ–‡ç« æ ‡é¢˜",
  "url": "æ–‡ç« é“¾æ¥",
  "source_name": "æ•°æ®æºåç§°",
  "language": "è¯­è¨€(en/zh)",
  "category": "åˆ†ç±»(news/research/tutorial/interview/product)",
  "summary": "æ–‡ç« æ‘˜è¦",
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
  "publish_time": "å‘å¸ƒæ—¶é—´",
  "fetch_time": "æŠ“å–æ—¶é—´"
}
```

## ğŸ”§ å¸¸è§é—®é¢˜

### Q: æŸäº›æºæŠ“å–å¤±è´¥ï¼Ÿ
A: éƒ¨åˆ†RSSæºå¯èƒ½æš‚æ—¶ä¸å¯ç”¨æˆ–æ ¼å¼æœ‰é—®é¢˜ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è·³è¿‡å¹¶ç»§ç»­æŠ“å–å…¶ä»–æºã€‚

### Q: å¦‚ä½•æ·»åŠ æ–°çš„æ•°æ®æºï¼Ÿ
A: ç¼–è¾‘ `src/sources/rss_sources.py` æ–‡ä»¶ï¼Œåœ¨ `RSS_SOURCES` å­—å…¸ä¸­æ·»åŠ æ–°æºã€‚

### Q: å¦‚ä½•ä¿®æ”¹æŠ“å–é¢‘ç‡ï¼Ÿ
A: ä½¿ç”¨ `scheduler.py` æ—¶é€šè¿‡ `--interval` å‚æ•°æŒ‡å®šï¼Œå¦‚ `--interval 2h` è¡¨ç¤ºæ¯2å°æ—¶ã€‚

### Q: æ•°æ®å­˜å‚¨åœ¨å“ªé‡Œï¼Ÿ
A: é»˜è®¤å­˜å‚¨åœ¨ `data/ai_news.json`ï¼Œå¯åœ¨ `config.yaml` ä¸­ä¿®æ”¹ã€‚

### Q: å¦‚ä½•æ¸…ç©ºå†å²æ•°æ®ï¼Ÿ
A: åˆ é™¤ `data/ai_news.json` æ–‡ä»¶å³å¯ã€‚

## ğŸ“ æ—¥å¿—æŸ¥çœ‹

æ—¥å¿—æ–‡ä»¶ä½ç½®ï¼š`logs/crawler.log`

```bash
# æŸ¥çœ‹æœ€æ–°æ—¥å¿—
tail -f logs/crawler.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep ERROR logs/crawler.log
```

## ğŸš€ è¿›é˜¶ä½¿ç”¨

### åªæŠ“å–ç ”ç©¶ç±»æ–‡ç« 

ä¿®æ”¹ `config.yaml`:
```yaml
filter:
  keywords:
    - paper
    - research
    - arxiv
    - è®ºæ–‡
    - ç ”ç©¶
```

### é›†æˆåˆ°å…¶ä»–ç³»ç»Ÿ

å¯ä»¥é€šè¿‡è¯»å–JSONæ–‡ä»¶æˆ–ä½¿ç”¨SQLiteæ•°æ®åº“é›†æˆåˆ°å…¶ä»–ç³»ç»Ÿï¼š

```python
import json

# è¯»å–æ•°æ®
with open('data/ai_news.json', 'r') as f:
    articles = json.load(f)

# å¤„ç†æ•°æ®
for article in articles:
    print(f"{article['title']} - {article['url']}")
```

## ğŸ“§ é€šçŸ¥åŠŸèƒ½ï¼ˆå¾…å¼€å‘ï¼‰

æœªæ¥ç‰ˆæœ¬å°†æ”¯æŒï¼š
- é‚®ä»¶é€šçŸ¥
- Webhooké€šçŸ¥
- å¾®ä¿¡/é’‰é’‰æœºå™¨äººé€šçŸ¥

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License
