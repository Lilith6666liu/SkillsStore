"""
AIèµ„è®¯æŠ“å–ç³»ç»Ÿ - æ•°æ®å¤„ç†æ¨¡å—

åŠŸèƒ½ï¼š
- å¯¹æŠ“å–çš„å†…å®¹è¿›è¡Œæ¸…æ´—ã€åˆ†ç±»ã€æ ¼å¼åŒ–
- æŒ‰ç±»åˆ«ç»„ç»‡æ•°æ®
- è‡ªåŠ¨å»é‡
- ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
"""

import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from collections import Counter
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import CATEGORIES, COMPANIES, OUTPUT_CONFIG
from models import (
    AINewsItem, 
    CategoryStats, 
    CompanyStats, 
    ReportData,
    DataStore
)


class NewsProcessor:
    """æ–°é—»å¤„ç†å™¨"""

    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•
        """
        self.data_dir = data_dir or os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
            "data"
        )
        self.store = DataStore(self.data_dir)

    def clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€å¸¸ç”¨æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿã€""''ã€ã€‘ï¼ˆï¼‰()\-â€”.,!?\'\"]', '', text)
        
        return text.strip()

    def categorize_item(self, item: AINewsItem) -> str:
        """
        è‡ªåŠ¨åˆ†ç±»æ–°é—»
        
        Args:
            item: æ–°é—»æ¡ç›®
            
        Returns:
            åˆ†ç±»åç§°
        """
        # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œåˆ†æ
        text = f"{item.title} {item.summary}".lower()
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŒ¹é…åˆ†æ•°
        scores = {}
        
        for category, info in CATEGORIES.items():
            score = 0
            
            # æ£€æŸ¥å…³é”®è¯
            for keyword in info.get("keywords_zh", []):
                if keyword in text:
                    score += 2
            
            for keyword in info.get("keywords_en", []):
                if keyword.lower() in text:
                    score += 2
            
            scores[category] = score
        
        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ç±»åˆ«
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)
        
        # é»˜è®¤è¿”å›"news"
        return "news"

    def extract_keywords(self, item: AINewsItem, max_keywords: int = 5) -> List[str]:
        """
        æå–å…³é”®è¯
        
        Args:
            item: æ–°é—»æ¡ç›®
            max_keywords: æœ€å¤§å…³é”®è¯æ•°
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # åˆå¹¶æ–‡æœ¬
        text = f"{item.title} {item.summary}"
        
        # ç®€å•å…³é”®è¯æå–ï¼ˆåŸºäºè¯é¢‘ï¼‰
        words = re.findall(r'[\w\u4e00-\u9fff]+', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯
        stopwords = set([
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 
            'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 
            'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'the', 'a', 'an', 'is', 'are', 'was',
            'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can',
            'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as',
            'and', 'or', 'but', 'if', 'then', 'this', 'that', 'these', 'those',
            'ai', 'artificial', 'intelligence', 'new', 'latest', 'news'
        ])
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter()
        for word in words:
            if len(word) >= 2 and word not in stopwords:
                word_counts[word] += 1
        
        # è¿”å›é«˜é¢‘è¯
        keywords = [word for word, count in word_counts.most_common(max_keywords)]
        
        return keywords

    def calculate_importance(self, item: AINewsItem) -> int:
        """
        è®¡ç®—é‡è¦æ€§è¯„åˆ† (1-10)
        
        Args:
            item: æ–°é—»æ¡ç›®
            
        Returns:
            é‡è¦æ€§è¯„åˆ†
        """
        score = 5  # åŸºç¡€åˆ†æ•°
        
        # å…¬å¸æƒé‡
        for company in item.companies:
            if company in COMPANIES:
                score += 1
        
        # ç±»åˆ«æƒé‡
        category_priority = CATEGORIES.get(item.category, {}).get("priority", 3)
        score += (7 - category_priority) * 0.5
        
        # æ¥æºæƒé‡
        important_sources = ["OpenAI Blog", "Google Blog", "Meta AI", "Microsoft Blog"]
        if item.source in important_sources:
            score += 2
        
        # å†…å®¹é•¿åº¦æƒé‡
        if len(item.summary) > 100:
            score += 1
        
        # é™åˆ¶åˆ†æ•°èŒƒå›´
        return min(10, max(1, int(score)))

    def process_items(self, items: List[AINewsItem]) -> List[AINewsItem]:
        """
        å¤„ç†æ–°é—»åˆ—è¡¨
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„æ–°é—»åˆ—è¡¨
        """
        processed_items = []
        
        for item in items:
            try:
                # æ¸…æ´—æ–‡æœ¬
                item.title = self.clean_text(item.title)
                item.summary = self.clean_text(item.summary)
                
                # é‡æ–°åˆ†ç±»
                item.category = self.categorize_item(item)
                
                # æå–å…³é”®è¯
                item.keywords = self.extract_keywords(item)
                
                # è®¡ç®—é‡è¦æ€§
                item.importance = self.calculate_importance(item)
                
                processed_items.append(item)
                
            except Exception as e:
                print(f"   âš  å¤„ç†å¤±è´¥: {e}")
                processed_items.append(item)
        
        print(f"   âœ“ å¤„ç†å®Œæˆ {len(processed_items)} æ¡æ–°é—»")
        
        return processed_items

    def filter_by_date(self, items: List[AINewsItem], days: int = 7) -> List[AINewsItem]:
        """
        æŒ‰æ—¥æœŸè¿‡æ»¤
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            days: å¤©æ•°
            
        Returns:
            è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
        """
        cutoff_date = datetime.now() - timedelta(days=days)
        
        filtered_items = []
        for item in items:
            try:
                pub_date = datetime.strptime(item.publish_time, "%Y-%m-%d %H:%M:%S")
                if pub_date >= cutoff_date:
                    filtered_items.append(item)
            except:
                # å¦‚æœæ— æ³•è§£ææ—¥æœŸï¼Œä¿ç•™è¯¥é¡¹ç›®
                filtered_items.append(item)
        
        return filtered_items

    def filter_by_category(self, items: List[AINewsItem], 
                          categories: List[str]) -> List[AINewsItem]:
        """
        æŒ‰ç±»åˆ«è¿‡æ»¤
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            categories: ç±»åˆ«åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
        """
        return [item for item in items if item.category in categories]

    def filter_by_source_type(self, items: List[AINewsItem], 
                             source_type: str) -> List[AINewsItem]:
        """
        æŒ‰æ¥æºç±»å‹è¿‡æ»¤
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            source_type: æ¥æºç±»å‹ (international/domestic)
            
        Returns:
            è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
        """
        return [item for item in items if item.source_type == source_type]

    def sort_by_importance(self, items: List[AINewsItem], 
                          ascending: bool = False) -> List[AINewsItem]:
        """
        æŒ‰é‡è¦æ€§æ’åº
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            ascending: å‡åº
            
        Returns:
            æ’åºåçš„æ–°é—»åˆ—è¡¨
        """
        return sorted(items, key=lambda x: x.importance, reverse=not ascending)

    def sort_by_date(self, items: List[AINewsItem], 
                    ascending: bool = False) -> List[AINewsItem]:
        """
        æŒ‰æ—¥æœŸæ’åº
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            ascending: å‡åº
            
        Returns:
            æ’åºåçš„æ–°é—»åˆ—è¡¨
        """
        return sorted(
            items, 
            key=lambda x: datetime.strptime(x.publish_time, "%Y-%m-%d %H:%M:%S"),
            reverse=not ascending
        )

    def deduplicate(self, items: List[AINewsItem]) -> List[AINewsItem]:
        """
        å»é‡ï¼ˆåŸºäºURLï¼‰
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–°é—»åˆ—è¡¨
        """
        seen_urls = set()
        unique_items = []
        
        for item in items:
            if item.url not in seen_urls:
                seen_urls.add(item.url)
                unique_items.append(item)
        
        return unique_items

    def group_by_category(self, items: List[AINewsItem]) -> Dict[str, List[AINewsItem]]:
        """
        æŒ‰ç±»åˆ«åˆ†ç»„
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            
        Returns:
            åˆ†ç»„åçš„å­—å…¸
        """
        groups = {}
        for item in items:
            if item.category not in groups:
                groups[item.category] = []
            groups[item.category].append(item)
        
        return groups

    def group_by_company(self, items: List[AINewsItem]) -> Dict[str, List[AINewsItem]]:
        """
        æŒ‰å…¬å¸åˆ†ç»„
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            
        Returns:
            åˆ†ç»„åçš„å­—å…¸
        """
        groups = {}
        for item in items:
            for company in item.companies:
                if company not in groups:
                    groups[company] = []
                groups[company].append(item)
        
        return groups

    def generate_report(self, items: List[AINewsItem], 
                       days: int = 7) -> ReportData:
        """
        ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            days: æ—¶é—´èŒƒå›´
            
        Returns:
            æŠ¥å‘Šæ•°æ®
        """
        # æŒ‰æ—¥æœŸè¿‡æ»¤
        filtered_items = self.filter_by_date(items, days)
        
        # ç±»åˆ«ç»Ÿè®¡
        category_counts = Counter(item.category for item in filtered_items)
        categories = []
        for cat, count in category_counts.most_common():
            latest_time = None
            for item in filtered_items:
                if item.category == cat:
                    latest_time = item.publish_time
                    break
            categories.append(CategoryStats(
                category=cat,
                count=count,
                latest_time=latest_time
            ))
        
        # å…¬å¸ç»Ÿè®¡
        company_counts = Counter()
        for item in filtered_items:
            company_counts.update(item.companies)
        
        companies = []
        for company, count in company_counts.most_common(10):
            source_type = COMPANIES.get(company, {}).get("type", "unknown")
            latest_time = None
            for item in filtered_items:
                if company in item.companies:
                    latest_time = item.publish_time
                    break
            companies.append(CompanyStats(
                company=company,
                count=count,
                source_type=source_type,
                latest_time=latest_time
            ))
        
        # æ¥æºç»Ÿè®¡
        international_count = len(self.filter_by_source_type(filtered_items, "international"))
        domestic_count = len(self.filter_by_source_type(filtered_items, "domestic"))
        
        # æœ€æ–°å’Œæœ€é‡è¦æ–°é—»
        latest_news = self.sort_by_date(filtered_items)[:10]
        top_news = self.sort_by_importance(filtered_items)[:10]
        
        # æ—¥æœŸèŒƒå›´
        date_range = f"æœ€è¿‘{days}å¤©"
        
        return ReportData(
            total_news=len(filtered_items),
            date_range=date_range,
            categories=categories,
            companies=companies,
            international_count=international_count,
            domestic_count=domestic_count,
            latest_news=latest_news,
            top_news=top_news
        )

    def save_processed_data(self, items: List[AINewsItem], 
                           prefix: str = "processed") -> Dict[str, str]:
        """
        ä¿å­˜å¤„ç†åçš„æ•°æ®
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            prefix: æ–‡ä»¶å‰ç¼€
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        saved_files = {}
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æŒ‰ç±»åˆ«ä¿å­˜
        by_category = self.group_by_category(items)
        for category, cat_items in by_category.items():
            filename = f"{prefix}_category_{category}_{timestamp}.json"
            filepath = os.path.join(self.data_dir, "by_category", filename)
            self.store.save_items(cat_items, os.path.join("by_category", filename))
            saved_files[f"category_{category}"] = filepath
        
        # æŒ‰å…¬å¸ä¿å­˜
        by_company = self.group_by_company(items)
        for company, comp_items in by_company.items():
            filename = f"{prefix}_company_{company}_{timestamp}.json"
            filepath = os.path.join(self.data_dir, "by_company", filename)
            self.store.save_items(comp_items, os.path.join("by_company", filename))
            saved_files[f"company_{company}"] = filepath
        
        # ä¿å­˜å®Œæ•´åˆ—è¡¨
        filename = f"{prefix}_all_{timestamp}.json"
        filepath = os.path.join(self.data_dir, filename)
        self.store.save_items(items, filename)
        saved_files["all"] = filepath
        
        # ä¿å­˜æœ€æ–°åˆ—è¡¨
        latest_filename = "latest.json"
        self.store.save_items(items[:OUTPUT_CONFIG["max_items_per_category"]], latest_filename)
        saved_files["latest"] = os.path.join(self.data_dir, latest_filename)
        
        print(f"\nğŸ’¾ æ•°æ®ä¿å­˜å®Œæˆ")
        for key, path in saved_files.items():
            print(f"   {key}: {path}")
        
        return saved_files


class DataSearcher:
    """æ•°æ®æœç´¢å™¨"""

    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–æœç´¢å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•
        """
        self.data_dir = data_dir or os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
            "data"
        )
        self.store = DataStore(self.data_dir)

    def search_items(self, items: List[AINewsItem], 
                    query: str = None,
                    categories: List[str] = None,
                    companies: List[str] = None,
                    source_type: str = None,
                    days: int = None) -> List[AINewsItem]:
        """
        æœç´¢æ–°é—»
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            query: æœç´¢å…³é”®è¯
            categories: ç±»åˆ«è¿‡æ»¤
            companies: å…¬å¸è¿‡æ»¤
            source_type: æ¥æºç±»å‹è¿‡æ»¤
            days: æ—¥æœŸè¿‡æ»¤
            
        Returns:
            è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
        """
        result = items
        
        # æŒ‰å…³é”®è¯æœç´¢
        if query:
            query_lower = query.lower()
            result = [
                item for item in result 
                if query_lower in item.title.lower() 
                or query_lower in item.summary.lower()
            ]
        
        # æŒ‰ç±»åˆ«è¿‡æ»¤
        if categories:
            result = [item for item in result if item.category in categories]
        
        # æŒ‰å…¬å¸è¿‡æ»¤
        if companies:
            result = [
                item for item in result 
                if any(c in item.companies for c in companies)
            ]
        
        # æŒ‰æ¥æºç±»å‹è¿‡æ»¤
        if source_type:
            result = [item for item in result if item.source_type == source_type]
        
        # æŒ‰æ—¥æœŸè¿‡æ»¤
        if days:
            processor = NewsProcessor()
            result = processor.filter_by_date(result, days)
        
        return result

    def load_latest(self) -> List[AINewsItem]:
        """åŠ è½½æœ€æ–°æ•°æ®"""
        return self.store.load_items("latest.json")

    def load_by_category(self, category: str) -> List[AINewsItem]:
        """æŒ‰ç±»åˆ«åŠ è½½æ•°æ®"""
        # è·å–æœ€æ–°çš„ç±»åˆ«æ–‡ä»¶
        category_dir = os.path.join(self.data_dir, "by_category")
        if not os.path.exists(category_dir):
            return []
        
        files = [f for f in os.listdir(category_dir) 
                if f.startswith("processed") and f"category_{category}" in f]
        
        if not files:
            return []
        
        # åŠ è½½æœ€æ–°æ–‡ä»¶
        files.sort(reverse=True)
        return self.store.load_items(os.path.join("by_category", files[0]))


def processing_demo():
    """å¤„ç†æ¼”ç¤º"""
    from searcher import NewsSearcher
    import asyncio
    
    async def demo():
        print("ğŸ§ª æ•°æ®å¤„ç†æµ‹è¯•")
        
        # æœç´¢æ–°é—»
        searcher = NewsSearcher()
        result = await searcher.search_latest_news(days=7, limit=10)
        
        # å¤„ç†æ–°é—»
        processor = NewsProcessor()
        processed_items = processor.process_items(result.items)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = processor.generate_report(processed_items)
        print(f"\nğŸ“Š ç»Ÿè®¡æŠ¥å‘Š")
        print(f"   æ€»æ–°é—»æ•°: {report.total_news}")
        print(f"   å›½é™…æ–°é—»: {report.international_count}")
        print(f"   å›½å†…æ–°é—»: {report.domestic_count}")
        print(f"\n   ç±»åˆ«åˆ†å¸ƒ:")
        for cat in report.categories:
            cat_name = CATEGORIES.get(cat.category, {}).get("name", cat.category)
            print(f"     - {cat_name}: {cat.count}")
        
        print(f"\n   çƒ­é—¨å…¬å¸:")
        for comp in report.companies[:5]:
            print(f"     - {comp.company}: {comp.count}")
        
        return processed_items
    
    return asyncio.run(demo())


if __name__ == "__main__":
    processing_demo()
