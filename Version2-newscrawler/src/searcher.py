"""
AIèµ„è®¯æŠ“å–ç³»ç»Ÿ - æœç´¢æ¨¡å—

åŠŸèƒ½ï¼š
- ä½¿ç”¨MCPå·¥å…·è¿›è¡Œå¹¶è¡Œæœç´¢
- æ”¯æŒæŒ‰å…³é”®è¯ã€æ—¶é—´èŒƒå›´ã€æ¥æºç­›é€‰
- æ”¯æŒä¸­è‹±æ–‡æœç´¢
"""

import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import (
    SEARCH_CONFIG, 
    SEARCH_KEYWORDS, 
    COMPANIES, 
    CATEGORIES,
    NEWS_SOURCES
)
from models import AINewsItem, SearchResult


class NewsSearcher:
    """AIæ–°é—»æœç´¢å™¨"""

    def __init__(self, use_mcp: bool = True):
        """
        åˆå§‹åŒ–æœç´¢å™¨
        
        Args:
            use_mcp: æ˜¯å¦ä½¿ç”¨MCPå·¥å…·
        """
        self.use_mcp = use_mcp
        self.mcp_client = None
        if use_mcp:
            try:
                # å°è¯•å¯¼å…¥MCPå·¥å…·
                from mcp.client import MCPClient
                self.mcp_client = MCPClient()
                print("âœ“ MCPå®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
            except ImportError:
                print("âš  MCPå·¥å…·æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢")
                self.use_mcp = False

    def _build_search_queries(self, keywords: List[str], days: int = 7, 
                              language: str = "all") -> List[Dict[str, Any]]:
        """
        æ„å»ºæœç´¢æŸ¥è¯¢åˆ—è¡¨
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            days: æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰
            language: è¯­è¨€ï¼ˆall/zh/enï¼‰
            
        Returns:
            æœç´¢æŸ¥è¯¢åˆ—è¡¨
        """
        queries = []
        date_str = datetime.now().strftime("%Y-%m-%d")
        
        for keyword in keywords:
            if language in ["all", "zh"]:
                # ä¸­æ–‡æœç´¢
                queries.append({
                    "query": f"{keyword} æœ€æ–°æ¶ˆæ¯",
                    "language": "zh",
                    "date_range": f"d{days}",
                    "num_results": SEARCH_CONFIG["max_results_per_query"]
                })
            
            if language in ["all", "en"]:
                # è‹±æ–‡æœç´¢
                queries.append({
                    "query": f"{keyword} latest news",
                    "language": "en",
                    "date_range": f"d{days}",
                    "num_results": SEARCH_CONFIG["max_results_per_query"]
                })
        
        return queries

    def _parse_search_result(self, result: Dict[str, Any], query: str) -> List[AINewsItem]:
        """
        è§£ææœç´¢ç»“æœ
        
        Args:
            result: æœç´¢ç»“æœ
            query: æœç´¢å…³é”®è¯
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        items = []
        
        # å°è¯•è§£æMCPæœç´¢ç»“æœ
        if "results" in result:
            for r in result["results"]:
                try:
                    # æå–å…³é”®ä¿¡æ¯
                    title = r.get("title", "")
                    url = r.get("url", "")
                    snippet = r.get("snippet", "")
                    source = r.get("source", "")
                    publish_time = r.get("publish_time", "")
                    
                    # å¦‚æœæ²¡æœ‰æ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                    if not publish_time:
                        publish_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    
                    # åˆ›å»ºæ–°é—»æ¡ç›®
                    item = AINewsItem(
                        title=title,
                        source=source or "Unknown",
                        source_type=self._detect_source_type(url),
                        category="news",  # é»˜è®¤ä¸ºæ–°é—»ï¼Œåç»­ä¼šé‡æ–°åˆ†ç±»
                        publish_time=publish_time,
                        url=url,
                        summary=snippet[:300] if snippet else "",
                        keywords=[query],
                    )
                    
                    # æ£€æµ‹æåˆ°çš„å…¬å¸
                    item.companies = self._detect_companies(title + " " + snippet)
                    
                    # æ£€æµ‹è¯­è¨€
                    item.language = self._detect_language(title)
                    
                    items.append(item)
                except Exception as e:
                    print(f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
                    continue
        
        return items

    def _detect_source_type(self, url: str) -> str:
        """æ£€æµ‹æ¥æºç±»å‹ï¼ˆå›½é™…/å›½å†…ï¼‰"""
        domestic_keywords = [
            "36kr.com", "huxiu.com", "leiphone.com", "jiqizhixin.com",
            "qbitai.com", "infoq.cn", "csdn.net", "baidu.com", "alibaba.com",
            "tencent.com", "bytedance.com", "163.com", "sina.com.cn"
        ]
        
        for keyword in domestic_keywords:
            if keyword in url.lower():
                return "domestic"
        
        return "international"

    def _detect_companies(self, text: str) -> List[str]:
        """æ£€æµ‹æ–‡æœ¬ä¸­æåˆ°çš„å…¬å¸"""
        companies = []
        text_lower = text.lower()
        
        for company, info in COMPANIES.items():
            # æ£€æŸ¥è‹±æ–‡å…³é”®è¯
            for keyword in info.get("keywords_en", []):
                if keyword.lower() in text_lower:
                    companies.append(company)
                    break
            
            # æ£€æŸ¥ä¸­æ–‡å…³é”®è¯
            for keyword in info.get("keywords_zh", []):
                if keyword in text:
                    if company not in companies:
                        companies.append(company)
                    break
        
        return companies

    def _detect_language(self, text: str) -> str:
        """æ£€æµ‹è¯­è¨€"""
        chinese_chars = 0
        english_words = 0
        
        for char in text:
            if '\u4e00' <= char <= '\u9fff':
                chinese_chars += 1
            elif char.isalpha():
                english_words += 1
        
        if chinese_chars > english_words:
            return "zh"
        elif english_words > chinese_chars:
            return "en"
        else:
            return "mixed"

    async def search(self, keywords: List[str], days: int = 7, 
                    language: str = "all", max_results: int = 50) -> SearchResult:
        """
        æœç´¢AIæ–°é—»
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            days: æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰
            language: è¯­è¨€
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æœç´¢ç»“æœ
        """
        print(f"\nğŸ” å¼€å§‹æœç´¢...")
        print(f"   å…³é”®è¯: {keywords}")
        print(f"   æ—¶é—´èŒƒå›´: æœ€è¿‘{days}å¤©")
        print(f"   è¯­è¨€: {language}")
        
        all_items = []
        
        if self.use_mcp:
            # ä½¿ç”¨MCPå·¥å…·æœç´¢
            queries = self._build_search_queries(keywords, days, language)
            
            try:
                # ä½¿ç”¨MCP batch_web_searchå·¥å…·
                from mcp.tools import batch_web_search
                
                # é™åˆ¶å¹¶å‘æ•°
                results = []
                for i in range(0, len(queries), SEARCH_CONFIG["max_concurrent_searches"]):
                    batch = queries[i:i + SEARCH_CONFIG["max_concurrent_searches"]]
                    batch_results = await batch_web_search(queries=batch)
                    results.extend(batch_results)
                
                # è§£æç»“æœ
                for query, result in zip(queries, results):
                    items = self._parse_search_result(result, query["query"])
                    all_items.extend(items)
                    
            except Exception as e:
                print(f"âš  MCPæœç´¢å¤±è´¥: {e}")
                print("   ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
                all_items = self._generate_mock_results(keywords)
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢
            print("   ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢...")
            all_items = self._generate_mock_results(keywords)
        
        # å»é‡
        unique_items = self._deduplicate_items(all_items)
        
        # é™åˆ¶ç»“æœæ•°é‡
        unique_items = unique_items[:max_results]
        
        print(f"   âœ“ æ‰¾åˆ° {len(unique_items)} æ¡ç›¸å…³èµ„è®¯")
        
        return SearchResult(
            query="; ".join(keywords),
            total_results=len(unique_items),
            items=unique_items
        )

    def _generate_mock_results(self, keywords: List[str]) -> List[AINewsItem]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæœç´¢ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        mock_news = [
            {
                "title": "OpenAIå‘å¸ƒGPT-5é¢„è§ˆç‰ˆï¼Œæ€§èƒ½æå‡æ˜¾è‘—",
                "source": "TechCrunch",
                "source_type": "international",
                "url": "https://techcrunch.com/2024/01/20/openai-gpt-5-preview/",
                "summary": "OpenAIä»Šæ—¥å®£å¸ƒæ¨å‡ºGPT-5é¢„è§ˆç‰ˆï¼Œæ–°æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›å’Œå¤šæ¨¡æ€å¤„ç†æ–¹é¢å®ç°é‡å¤§çªç ´...",
                "companies": ["OpenAI"],
                "category": "product",
            },
            {
                "title": "è°·æ­ŒDeepMindå‘å¸ƒGemini 1.5 Proï¼Œæ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡",
                "source": "The Verge",
                "source_type": "international",
                "url": "https://www.theverge.com/2024/01/19/google-gemini-1-5-pro/",
                "summary": "è°·æ­ŒDeepMindå‘å¸ƒæ–°ä¸€ä»£Gemini 1.5 Proæ¨¡å‹ï¼Œæ”¯æŒæœ€é«˜200ä¸‡tokençš„ä¸Šä¸‹æ–‡çª—å£...",
                "companies": ["Google"],
                "category": "product",
            },
            {
                "title": "ç™¾åº¦æ–‡å¿ƒä¸€è¨€4.0å‘å¸ƒï¼Œä¸­æ–‡ç†è§£èƒ½åŠ›å†å‡çº§",
                "source": "36æ°ª",
                "source_type": "domestic",
                "url": "https://36kr.com/p/123456",
                "summary": "ç™¾åº¦ä»Šæ—¥å‘å¸ƒæ–‡å¿ƒä¸€è¨€4.0ï¼Œæ–°ç‰ˆæœ¬åœ¨ä¸­æ–‡è¯­ä¹‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥...",
                "companies": ["ç™¾åº¦"],
                "category": "product",
            },
            {
                "title": "Anthropicå‘å¸ƒClaude 3ç³»åˆ—æ¨¡å‹ï¼Œæ€§èƒ½è¶…è¶ŠGPT-4",
                "source": "Wired",
                "source_type": "international",
                "url": "https://wired.com/2024/01/18/anthropic-claude-3/",
                "summary": "Anthropicæ¨å‡ºClaude 3ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬Haikuã€Sonnetå’ŒOpusä¸‰ä¸ªç‰ˆæœ¬ï¼Œå…¶ä¸­Opusç‰ˆæœ¬åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¶…è¶ŠGPT-4...",
                "companies": ["Anthropic"],
                "category": "product",
            },
            {
                "title": "é˜¿é‡Œäº‘é€šä¹‰åƒé—®Qwen2-VLå‘å¸ƒï¼Œæ”¯æŒè§†è§‰ç†è§£",
                "source": "è™å—…ç½‘",
                "source_type": "domestic",
                "url": "https://www.huxiu.com/article/789012",
                "summary": "é˜¿é‡Œäº‘å‘å¸ƒé€šä¹‰åƒé—®Qwen2-VLè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨å›¾åƒç†è§£å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚...",
                "companies": ["é˜¿é‡Œå·´å·´"],
                "category": "product",
            },
            {
                "title": "å­—èŠ‚è·³åŠ¨è±†åŒ…å¤§æ¨¡å‹APIæ­£å¼å¼€æ”¾ï¼Œæ”¯æŒ128Kä¸Šä¸‹æ–‡",
                "source": "æœºå™¨ä¹‹å¿ƒ",
                "source_type": "domestic",
                "url": "https://www.jiqizhixin.com/articles/2024-01-17",
                "summary": "å­—èŠ‚è·³åŠ¨å®£å¸ƒè±†åŒ…å¤§æ¨¡å‹APIæ­£å¼å¼€æ”¾ï¼Œæ”¯æŒæœ€é«˜128Kçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œä¼ä¸šç”¨æˆ·å¯ç”³è¯·è°ƒç”¨...",
                "companies": ["å­—èŠ‚è·³åŠ¨"],
                "category": "product",
            },
            {
                "title": "Metaå‘å¸ƒLlama 3.1 405Bï¼Œå¼€æºæ¨¡å‹æ€§èƒ½åˆ›æ–°é«˜",
                "source": "Meta AI Blog",
                "source_type": "international",
                "url": "https://ai.meta.com/blog/llama-3-1-405B/",
                "summary": "Metaå‘å¸ƒLlama 3.1 405Bå‚æ•°å¼€æºæ¨¡å‹ï¼Œåœ¨å¤šé¡¹è¯„æµ‹ä¸­è¾¾åˆ°é—­æºæ¨¡å‹æ°´å¹³...",
                "companies": ["Meta"],
                "category": "research",
            },
            {
                "title": "æ™ºè°±AIå‘å¸ƒGLM-4ç³»åˆ—æ¨¡å‹ï¼Œæ”¯æŒ128Kä¸Šä¸‹æ–‡",
                "source": "é‡å­ä½",
                "source_type": "domestic",
                "url": "https://www.qbitai.com/article/345678",
                "summary": "æ™ºè°±AIå‘å¸ƒGLM-4ç³»åˆ—æ¨¡å‹ï¼Œæ–°æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°çªå‡º...",
                "companies": ["æ™ºè°±AI"],
                "category": "product",
            },
            {
                "title": "å¾®è½¯Copilotä¼ä¸šç‰ˆå‘å¸ƒï¼Œé›†æˆGPT-4 Turbo",
                "source": "Microsoft Blog",
                "source_type": "international",
                "url": "https://blogs.microsoft.com/blog/2024/01/16/",
                "summary": "å¾®è½¯å®£å¸ƒCopilotä¼ä¸šç‰ˆæ­£å¼å‘å¸ƒï¼Œæ–°ç‰ˆæœ¬é›†æˆGPT-4 Turboï¼Œæ”¯æŒæ›´å¼ºçš„ä¼ä¸šçº§AIåº”ç”¨...",
                "companies": ["Microsoft"],
                "category": "product",
            },
            {
                "title": "æœˆä¹‹æš—é¢Kimiæ™ºèƒ½åŠ©æ‰‹å‡çº§ï¼Œæ”¯æŒ200Kä¸Šä¸‹æ–‡",
                "source": "36æ°ª",
                "source_type": "domestic",
                "url": "https://36kr.com/p/901234",
                "summary": "æœˆä¹‹æš—é¢å®£å¸ƒKimiæ™ºèƒ½åŠ©æ‰‹å‡çº§è‡³æ”¯æŒ200Kä¸Šä¸‹æ–‡çª—å£ï¼Œå¯å¤„ç†æ›´é•¿çš„æ–‡æ¡£...",
                "companies": ["æœˆä¹‹æš—é¢"],
                "category": "product",
            },
            {
                "title": "æ–¯å¦ç¦å¤§å­¦å‘å¸ƒæœ€æ–°AIç ”ç©¶ï¼Œæ­ç¤ºå¤§æ¨¡å‹æ¶Œç°èƒ½åŠ›",
                "source": "MIT Technology Review",
                "source_type": "international",
                "url": "https://technologyreview.com/2024/01/15/",
                "summary": "æ–¯å¦ç¦å¤§å­¦ç ”ç©¶å›¢é˜Ÿå‘å¸ƒæœ€æ–°è®ºæ–‡ï¼Œæ·±å…¥æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹çš„æ¶Œç°èƒ½åŠ›åŠå…¶å·¥ä½œæœºåˆ¶...",
                "companies": [],
                "category": "research",
            },
            {
                "title": "OpenAI CEO Altmanè®¿è°ˆï¼šAIå®‰å…¨ä¸æœªæ¥å±•æœ›",
                "source": "The New York Times",
                "source_type": "international",
                "url": "https://nytimes.com/2024/01/14/",
                "summary": "OpenAI CEO Sam Altmanæ¥å—ä¸“è®¿ï¼Œåˆ†äº«å¯¹AIå®‰å…¨ã€ç›‘ç®¡å’Œæœªæ¥å‘å±•çš„è§è§£...",
                "companies": ["OpenAI"],
                "category": "interview",
            },
        ]
        
        items = []
        for news in mock_news:
            item = AINewsItem(
                title=news["title"],
                source=news["source"],
                source_type=news["source_type"],
                category=news["category"],
                publish_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                url=news["url"],
                summary=news["summary"],
                keywords=keywords,
                companies=news["companies"],
                language="zh" if news["source_type"] == "domestic" else "en",
            )
            items.append(item)
        
        return items

    def _deduplicate_items(self, items: List[AINewsItem]) -> List[AINewsItem]:
        """å»é‡"""
        seen_urls = set()
        unique_items = []
        
        for item in items:
            if item.url not in seen_urls:
                seen_urls.add(item.url)
                unique_items.append(item)
        
        return unique_items

    def search_by_companies(self, companies: List[str], days: int = 7) -> SearchResult:
        """
        æŒ‰å…¬å¸æœç´¢æ–°é—»
        
        Args:
            companies: å…¬å¸åˆ—è¡¨
            days: æ—¶é—´èŒƒå›´
            
        Returns:
            æœç´¢ç»“æœ
        """
        keywords = []
        for company in companies:
            if company in SEARCH_KEYWORDS["companies"]:
                keywords.extend(SEARCH_KEYWORDS["companies"][company].get("zh", []))
                keywords.extend(SEARCH_KEYWORDS["companies"][company].get("en", []))
        
        return self.search(keywords, days, language="all")

    def search_by_category(self, category: str, days: int = 7) -> SearchResult:
        """
        æŒ‰ç±»åˆ«æœç´¢æ–°é—»
        
        Args:
            category: ç±»åˆ«
            days: æ—¶é—´èŒƒå›´
            
        Returns:
            æœç´¢ç»“æœ
        """
        if category not in SEARCH_KEYWORDS["categories"]:
            print(f"âš  æœªçŸ¥ç±»åˆ«: {category}")
            return SearchResult(query=category, total_results=0, items=[])
        
        keywords = []
        keywords.extend(SEARCH_KEYWORDS["categories"][category].get("zh", []))
        keywords.extend(SEARCH_KEYWORDS["categories"][category].get("en", []))
        
        return self.search(keywords, days, language="all")

    def search_latest_news(self, days: int = 7, limit: int = 50) -> SearchResult:
        """
        æœç´¢æœ€æ–°AIæ–°é—»
        
        Args:
            days: æ—¶é—´èŒƒå›´
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        keywords = []
        keywords.extend(SEARCH_KEYWORDS["general"]["zh"])
        keywords.extend(SEARCH_KEYWORDS["general"]["en"])
        
        return self.search(keywords, days, language="all", max_results=limit)

    def search_international_news(self, days: int = 7, limit: int = 30) -> SearchResult:
        """
        æœç´¢å›½é™…AIæ–°é—»
        
        Args:
            days: æ—¶é—´èŒƒå›´
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        keywords = SEARCH_KEYWORDS["general"]["en"]
        return self.search(list(keywords), days, language="en", max_results=limit)

    def search_domestic_news(self, days: int = 7, limit: int = 30) -> SearchResult:
        """
        æœç´¢å›½å†…AIæ–°é—»
        
        Args:
            days: æ—¶é—´èŒƒå›´
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        keywords = SEARCH_KEYWORDS["general"]["zh"]
        return self.search(list(keywords), days, language="zh", max_results=limit)


# å¼‚æ­¥æ”¯æŒ
import asyncio


async def async_search_demo():
    """å¼‚æ­¥æœç´¢æ¼”ç¤º"""
    searcher = NewsSearcher()
    result = await searcher.search_latest_news(days=7, limit=10)
    print(f"\næ‰¾åˆ° {result.total_results} æ¡æ–°é—»")
    for item in result.items[:3]:
        print(f"  - {item.title}")
        print(f"    æ¥æº: {item.source} | ç±»åˆ«: {item.category}")
    return result


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    result = asyncio.run(async_search_demo())
