"""
AIèµ„è®¯æŠ“å–ç³»ç»Ÿ - ä¸»ç¨‹åº

åŠŸèƒ½ï¼š
- æ•´åˆæœç´¢ã€æå–ã€å¤„ç†åŠŸèƒ½
- æä¾›å‘½ä»¤è¡Œæ¥å£
- æ”¯æŒå¤šç§è¿è¡Œæ¨¡å¼
"""

import asyncio
import argparse
import sys
import os
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import CATEGORIES, COMPANIES, DEFAULT_DAYS, OUTPUT_CONFIG
from models import AINewsItem, SearchResult, DataStore
from searcher import NewsSearcher
from extractor import ContentExtractor, IncrementalExtractor
from processor import NewsProcessor, DataSearcher


class AINewsScraper:
    """AIèµ„è®¯æŠ“å–å™¨ä¸»ç±»"""

    def __init__(self, use_mcp: bool = True):
        """
        åˆå§‹åŒ–æŠ“å–å™¨
        
        Args:
            use_mcp: æ˜¯å¦ä½¿ç”¨MCPå·¥å…·
        """
        self.use_mcp = use_mcp
        self.searcher = NewsSearcher(use_mcp=use_mcp)
        self.extractor = ContentExtractor(use_mcp=use_mcp)
        self.processor = NewsProcessor()
        self.searcher_incremental = IncrementalExtractor(self.extractor)
        self.data_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
            "data"
        )
        self.store = DataStore(self.data_dir)
        
        # åŠ è½½å·²å¤„ç†URL
        self.searcher_incremental.load_processed_urls(
            os.path.join(self.data_dir, "index", "processed_urls.json")
        )

    async def fetch_latest_news(self, days: int = DEFAULT_DAYS, 
                                limit: int = 50,
                                extract_content: bool = False,
                                save: bool = True) -> List[AINewsItem]:
        """
        è·å–æœ€æ–°AIæ–°é—»
        
        Args:
            days: æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰
            limit: é™åˆ¶æ•°é‡
            extract_content: æ˜¯å¦æå–è¯¦ç»†å†…å®¹
            save: æ˜¯å¦ä¿å­˜æ•°æ®
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        print("=" * 60)
        print("ğŸ¤– AIèµ„è®¯å®æ—¶æŠ“å–ç³»ç»Ÿ")
        print("=" * 60)
        print(f"\nğŸ“¡ æ¨¡å¼: è·å–æœ€æ–°AIèµ„è®¯")
        print(f"   æ—¶é—´èŒƒå›´: æœ€è¿‘{days}å¤©")
        print(f"   é™åˆ¶æ•°é‡: {limit}æ¡")
        print(f"   æå–è¯¦æƒ…: {'æ˜¯' if extract_content else 'å¦'}")
        print(f"   è‡ªåŠ¨ä¿å­˜: {'æ˜¯' if save else 'å¦'}")
        
        # 1. æœç´¢æ–°é—»
        search_result = await self.searcher.search_latest_news(days=days, limit=limit)
        
        if not search_result.items:
            print("\nâš  æœªæ‰¾åˆ°ç›¸å…³æ–°é—»")
            return []
        
        # 2. æå–è¯¦ç»†å†…å®¹ï¼ˆå¯é€‰ï¼‰
        if extract_content:
            items = await self.extractor.extract_batch(search_result.items)
        else:
            items = search_result.items
        
        # 3. å¤„ç†æ•°æ®
        processed_items = self.processor.process_items(items)
        
        # 4. ä¿å­˜æ•°æ®
        if save:
            self.processor.save_processed_data(processed_items)
            # ä¿å­˜å·²å¤„ç†URL
            self.searcher_incremental.save_processed_urls(
                os.path.join(self.data_dir, "index", "processed_urls.json")
            )
        
        return processed_items

    async def fetch_by_company(self, companies: List[str],
                              days: int = DEFAULT_DAYS,
                              limit: int = 30) -> List[AINewsItem]:
        """
        æŒ‰å…¬å¸è·å–æ–°é—»
        
        Args:
            companies: å…¬å¸åˆ—è¡¨
            days: æ—¶é—´èŒƒå›´
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        print("\n" + "=" * 60)
        print(f"ğŸ¢ æ¨¡å¼: è·å–æŒ‡å®šå…¬å¸åŠ¨æ€")
        print(f"   å…¬å¸: {', '.join(companies)}")
        print(f"   æ—¶é—´èŒƒå›´: æœ€è¿‘{days}å¤©")
        
        search_result = await self.searcher.search_by_companies(companies, days)
        items = self.processor.process_items(search_result.items)
        
        return items

    async def fetch_by_category(self, category: str,
                               days: int = DEFAULT_DAYS,
                               limit: int = 30) -> List[AINewsItem]:
        """
        æŒ‰ç±»åˆ«è·å–æ–°é—»
        
        Args:
            category: ç±»åˆ«
            days: æ—¶é—´èŒƒå›´
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        print("\n" + "=" * 60)
        print(f"ğŸ“ æ¨¡å¼: è·å–{category}ç±»èµ„è®¯")
        
        search_result = await self.searcher.search_by_category(category, days)
        items = self.processor.process_items(search_result.items)
        
        return items

    async def fetch_international_news(self, days: int = DEFAULT_DAYS,
                                       limit: int = 30) -> List[AINewsItem]:
        """è·å–å›½é™…AIæ–°é—»"""
        print("\n" + "=" * 60)
        print("ğŸŒ æ¨¡å¼: è·å–å›½é™…AIèµ„è®¯")
        
        search_result = await self.searcher.search_international_news(days, limit)
        items = self.processor.process_items(search_result.items)
        
        return items

    async def fetch_domestic_news(self, days: int = DEFAULT_DAYS,
                                  limit: int = 30) -> List[AINewsItem]:
        """è·å–å›½å†…AIæ–°é—»"""
        print("\n" + "=" * 60)
        print("ğŸ‡¨ğŸ‡³ æ¨¡å¼: è·å–å›½å†…AIèµ„è®¯")
        
        search_result = await self.searcher.search_domestic_news(days, limit)
        items = self.processor.process_items(search_result.items)
        
        return items

    async def custom_search(self, keywords: List[str],
                           days: int = DEFAULT_DAYS,
                           limit: int = 30) -> List[AINewsItem]:
        """
        è‡ªå®šä¹‰æœç´¢
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            days: æ—¶é—´èŒƒå›´
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        print("\n" + "=" * 60)
        print("ğŸ” æ¨¡å¼: è‡ªå®šä¹‰æœç´¢")
        print(f"   å…³é”®è¯: {', '.join(keywords)}")
        
        search_result = await self.searcher.search(keywords, days, language="all", max_results=limit)
        items = self.processor.process_items(search_result.items)
        
        return items

    def display_news(self, items: List[AINewsItem], 
                    max_items: int = 20,
                    show_content: bool = False):
        """
        æ˜¾ç¤ºæ–°é—»åˆ—è¡¨
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            max_items: æœ€å¤§æ˜¾ç¤ºæ•°é‡
            show_content: æ˜¾ç¤ºè¯¦ç»†å†…å®¹
        """
        print(f"\nğŸ“° å…±æ‰¾åˆ° {len(items)} æ¡èµ„è®¯")
        print("-" * 60)
        
        display_items = items[:max_items]
        
        for i, item in enumerate(display_items, 1):
            # ç±»åˆ«åç§°
            cat_name = CATEGORIES.get(item.category, {}).get("name", item.category)
            
            # æ¥æºç±»å‹å›¾æ ‡
            type_icon = "ğŸŒ" if item.source_type == "international" else "ğŸ‡¨ğŸ‡³"
            
            # é‡è¦æ€§æ˜Ÿçº§
            stars = "â˜…" * item.importance + "â˜†" * (10 - item.importance)
            
            print(f"\n{i}. {item.title}")
            print(f"   {type_icon} æ¥æº: {item.source} | ç±»åˆ«: {cat_name}")
            print(f"   ğŸ“… æ—¶é—´: {item.publish_time}")
            print(f"   â­ é‡è¦: {stars[:5]}")
            print(f"   ğŸ”— é“¾æ¥: {item.url}")
            print(f"   ğŸ“ æ‘˜è¦: {item.summary[:200]}...")
            
            if show_content and item.content:
                print(f"\n   ğŸ“„ å†…å®¹:")
                for line in item.content[:500].split("\n"):
                    print(f"      {line}")
            
            # æåˆ°çš„å…¬å¸
            if item.companies:
                print(f"   ğŸ¢ å…¬å¸: {', '.join(item.companies)}")
            
            print("-" * 60)

    def display_report(self, items: List[AINewsItem], days: int = DEFAULT_DAYS):
        """
        æ˜¾ç¤ºç»Ÿè®¡æŠ¥å‘Š
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            days: æ—¶é—´èŒƒå›´
        """
        report = self.processor.generate_report(items, days)
        
        print("\n" + "=" * 60)
        print("ğŸ“Š AIèµ„è®¯ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 60)
        
        print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
        print(f"   èµ„è®¯æ€»æ•°: {report.total_news}")
        print(f"   æ—¶é—´èŒƒå›´: {report.date_range}")
        print(f"   å›½é™…åŠ¨æ€: {report.international_count} æ¡")
        print(f"   å›½å†…åŠ¨æ€: {report.domestic_count} æ¡")
        
        print(f"\nğŸ“‚ ç±»åˆ«åˆ†å¸ƒ")
        for cat in report.categories:
            cat_name = CATEGORIES.get(cat.category, {}).get("name", cat.category)
            bar = "â–ˆ" * (cat.count * 50 // max(report.total_news, 1))
            print(f"   {cat_name:10s}: {cat.count:3d} {bar}")
        
        print(f"\nğŸ¢ çƒ­é—¨å…¬å¸")
        for comp in report.companies[:10]:
            icon = "ğŸŒ" if comp.source_type == "international" else "ğŸ‡¨ğŸ‡³"
            print(f"   {icon} {comp.company:10s}: {comp.count:3d} æ¡åŠ¨æ€")
        
        print(f"\nğŸ”¥ é‡è¦åŠ¨æ€ TOP 10")
        for i, item in enumerate(report.top_news[:10], 1):
            cat_name = CATEGORIES.get(item.category, {}).get("name", item.category)
            print(f"   {i:2d}. [{cat_name}] {item.title[:50]}")
            print(f"       æ¥æº: {item.source} | é‡è¦åº¦: {item.importance}/10")

    def export_data(self, items: List[AINewsItem], 
                   format: str = "json",
                   filename: str = None):
        """
        å¯¼å‡ºæ•°æ®
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            format: æ ¼å¼ï¼ˆjson/csvï¼‰
            filename: æ–‡ä»¶å
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ai_news_{timestamp}.{format}"
        
        filepath = os.path.join(self.data_dir, filename)
        
        if format == "json":
            data = [item.to_dict() for item in items]
            with open(filepath, 'w', encoding='utf-8') as f:
                import json
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… æ•°æ®å·²å¯¼å‡º: {filepath}")
        
        elif format == "csv":
            import csv
            with open(filepath, 'w', encoding='utf-8-sig', newline='') as f:
                if items:
                    writer = csv.DictWriter(f, fieldnames=items[0].to_dict().keys())
                    writer.writeheader()
                    for item in items:
                        writer.writerow(item.to_dict())
            print(f"\nâœ… æ•°æ®å·²å¯¼å‡º: {filepath}")
        
        else:
            print(f"\nâš  ä¸æ”¯æŒçš„æ ¼å¼: {format}")


async def run_demo():
    """è¿è¡Œæ¼”ç¤º"""
    scraper = AINewsScraper()
    
    # 1. è·å–æœ€æ–°èµ„è®¯
    items = await scraper.fetch_latest_news(days=7, limit=15)
    
    if items:
        # 2. æ˜¾ç¤ºç»Ÿè®¡æŠ¥å‘Š
        scraper.display_report(items)
        
        # 3. æ˜¾ç¤ºæ–°é—»åˆ—è¡¨
        scraper.display_news(items, max_items=10)
        
        # 4. å¯¼å‡ºæ•°æ®
        scraper.export_data(items, filename="ai_news_demo.json")
    
    return items


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="AIèµ„è®¯å®æ—¶æŠ“å–ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python main.py --mode latest                    # è·å–æœ€æ–°AIèµ„è®¯
  python main.py --mode company --companies OpenAI Google  # è·å–æŒ‡å®šå…¬å¸åŠ¨æ€
  python main.py --mode category --category news  # è·å–æ–°é—»ç±»åˆ«
  python main.py --mode international             # è·å–å›½é™…AIèµ„è®¯
  python main.py --mode domestic                  # è·å–å›½å†…AIèµ„è®¯
  python main.py --mode search --keywords "GPT-5"  # è‡ªå®šä¹‰æœç´¢
        """
    )
    
    parser.add_argument(
        "--mode", 
        type=str, 
        default="latest",
        choices=["latest", "company", "category", "international", "domestic", "search", "demo", "report"],
        help="è¿è¡Œæ¨¡å¼"
    )
    
    parser.add_argument(
        "--keywords", 
        type=str, 
        nargs="+",
        default=[],
        help="æœç´¢å…³é”®è¯"
    )
    
    parser.add_argument(
        "--companies",
        type=str,
        nargs="+",
        default=[],
        help="æŒ‡å®šå…¬å¸"
    )
    
    parser.add_argument(
        "--category",
        type=str,
        default="news",
        help="æ–°é—»ç±»åˆ«"
    )
    
    parser.add_argument(
        "--days",
        type=int,
        default=DEFAULT_DAYS,
        help=f"æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤{DEFAULT_DAYS}å¤©"
    )
    
    parser.add_argument(
        "--limit",
        type=int,
        default=50,
        help="æœ€å¤§æ•°é‡é™åˆ¶"
    )
    
    parser.add_argument(
        "--extract",
        action="store_true",
        help="æ˜¯å¦æå–è¯¦ç»†å†…å®¹"
    )
    
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="ä¸ä¿å­˜æ•°æ®"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        default="json",
        choices=["json", "csv"],
        help="è¾“å‡ºæ ¼å¼"
    )
    
    parser.add_argument(
        "--display",
        type=str,
        choices=["news", "report", "all"],
        default="all",
        help="æ˜¾ç¤ºå†…å®¹"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæŠ“å–å™¨
    scraper = AINewsScraper()
    
    # è¿è¡Œ
    async def run():
        items = []
        
        if args.mode == "latest":
            items = await scraper.fetch_latest_news(
                days=args.days, 
                limit=args.limit,
                extract_content=args.extract,
                save=not args.no_save
            )
        
        elif args.mode == "company":
            if not args.companies:
                print("âš  è¯·æŒ‡å®šå…¬å¸åç§°")
                return
            items = await scraper.fetch_by_company(
                args.companies, 
                days=args.days, 
                limit=args.limit
            )
        
        elif args.mode == "category":
            items = await scraper.fetch_by_category(
                args.category,
                days=args.days,
                limit=args.limit
            )
        
        elif args.mode == "international":
            items = await scraper.fetch_international_news(
                days=args.days,
                limit=args.limit
            )
        
        elif args.mode == "domestic":
            items = await scraper.fetch_domestic_news(
                days=args.days,
                limit=args.limit
            )
        
        elif args.mode == "search":
            if not args.keywords:
                print("âš  è¯·æŒ‡å®šæœç´¢å…³é”®è¯")
                return
            items = await scraper.custom_search(
                args.keywords,
                days=args.days,
                limit=args.limit
            )
        
        elif args.mode == "demo":
            items = await run_demo()
        
        elif args.mode == "report":
            items = await scraper.fetch_latest_news(
                days=args.days,
                limit=args.limit,
                extract_content=False,
                save=False
            )
        
        # æ˜¾ç¤ºç»“æœ
        if items:
            if args.display in ["report", "all"]:
                scraper.display_report(items, days=args.days)
            
            if args.display in ["news", "all"]:
                scraper.display_news(items, max_items=args.limit)
            
            if not args.no_save:
                scraper.export_data(items, format=args.output)
        
        return items
    
    # è¿è¡Œ
    items = asyncio.run(run())
    
    print("\n" + "=" * 60)
    print("âœ… æ‰§è¡Œå®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    main()
