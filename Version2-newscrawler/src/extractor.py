"""
AIèµ„è®¯æŠ“å–ç³»ç»Ÿ - å†…å®¹æå–æ¨¡å—

åŠŸèƒ½ï¼š
- ä½¿ç”¨MCPå·¥å…·æå–ç½‘é¡µè¯¦ç»†å†…å®¹
- æ™ºèƒ½æå–æ ‡é¢˜ã€æ¥æºã€æ—¶é—´ã€æ‘˜è¦ã€é“¾æ¥
- æ”¯æŒæ‰¹é‡å¤„ç†å’Œå¢é‡æ›´æ–°
"""

import asyncio
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import EXTRACT_CONFIG
from models import AINewsItem


class ContentExtractor:
    """å†…å®¹æå–å™¨"""

    def __init__(self, use_mcp: bool = True):
        """
        åˆå§‹åŒ–å†…å®¹æå–å™¨
        
        Args:
            use_mcp: æ˜¯å¦ä½¿ç”¨MCPå·¥å…·
        """
        self.use_mcp = use_mcp
        self.cache = {}  # ç®€å•ç¼“å­˜

    async def extract_from_url(self, url: str, prompt: str = None) -> Dict[str, Any]:
        """
        ä»URLæå–å†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            prompt: æå–æç¤ºè¯
            
        Returns:
            æå–çš„å†…å®¹
        """
        if url in self.cache:
            print(f"   âœ“ ä½¿ç”¨ç¼“å­˜: {url[:50]}...")
            return self.cache[url]
        
        if self.use_mcp:
            try:
                # ä½¿ç”¨MCP extract_content_from_websiteså·¥å…·
                from mcp.tools import extract_content_from_websites
                
                if prompt is None:
                    prompt = """è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. æ–‡ç« æ ‡é¢˜
2. å‘å¸ƒæ—¶é—´å’Œæ¥æº
3. ä¸»è¦å†…å®¹æ‘˜è¦ï¼ˆ200å­—ä»¥å†…ï¼‰
4. å…³é”®è§‚ç‚¹å’Œç»“è®º
5. æåˆ°çš„å…¬å¸æˆ–äº§å“
"""
                
                result = await extract_content_from_websites(
                    tasks=[{
                        "url": url,
                        "prompt": prompt,
                    }]
                )
                
                if result and len(result) > 0:
                    extracted = result[0]
                    self.cache[url] = extracted
                    return extracted
                    
            except Exception as e:
                print(f"   âš  MCPæå–å¤±è´¥: {e}")
                # è¿”å›æ¨¡æ‹Ÿæ•°æ®
                return self._generate_mock_extraction(url)

        # è¿”å›æ¨¡æ‹Ÿæ•°æ®
        return self._generate_mock_extraction(url)

    async def extract_batch(self, items: List[AINewsItem], 
                           show_progress: bool = True) -> List[AINewsItem]:
        """
        æ‰¹é‡æå–å†…å®¹
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            show_progress: æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            æ›´æ–°åçš„æ–°é—»åˆ—è¡¨
        """
        if show_progress:
            print(f"\nğŸ“„ å¼€å§‹æå–è¯¦ç»†å†…å®¹...")
            print(f"   å…± {len(items)} æ¡æ–°é—»")
        
        updated_items = []
        
        for i, item in enumerate(items):
            if show_progress:
                print(f"   è¿›åº¦: {i+1}/{len(items)}", end="\r")
            
            try:
                # æå–å†…å®¹
                extracted = await self.extract_from_url(item.url)
                
                # æ›´æ–°æ–°é—»æ¡ç›®
                if extracted:
                    # æ›´æ–°æ‘˜è¦
                    if "summary" in extracted and extracted["summary"]:
                        item.summary = extracted["summary"]
                    
                    # æ›´æ–°å‘å¸ƒæ—¶é—´
                    if "publish_time" in extracted and extracted["publish_time"]:
                        item.publish_time = extracted["publish_time"]
                    
                    # æå–è¯¦ç»†å†…å®¹
                    if "content" in extracted and extracted["content"]:
                        item.content = extracted["content"][:EXTRACT_CONFIG["max_content_length"]]
                    
                    # æ›´æ–°å…³é”®è¯
                    if "keywords" in extracted and extracted["keywords"]:
                        item.keywords = list(set(item.keywords + extracted["keywords"]))
                
                updated_items.append(item)
                
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(0.5)
                
            except Exception as e:
                print(f"   âš  æå–å¤±è´¥: {e}")
                updated_items.append(item)
        
        if show_progress:
            print(f"   âœ“ å®Œæˆæå– {len(updated_items)} æ¡æ–°é—»")
        
        return updated_items

    def _generate_mock_extraction(self, url: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæå–ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        return {
            "title": "AIæ–°é—»è¯¦ç»†æŠ¥é“",
            "publish_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source": "æ¨¡æ‹Ÿæ¥æº",
            "summary": "è¿™æ˜¯æ ¹æ®URLç”Ÿæˆçš„æ¨¡æ‹Ÿæ‘˜è¦å†…å®¹ã€‚å®é™…ä½¿ç”¨æ—¶ä¼šä»ç½‘é¡µä¸­æå–çœŸå®å†…å®¹ã€‚",
            "content": """è¿™æ˜¯æ–‡ç« çš„è¯¦ç»†å†…å®¹éƒ¨åˆ†ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šåŒ…å«ä»ç½‘é¡µä¸­æå–çš„å®Œæ•´æ–‡ç« å†…å®¹ã€‚
            
æ–‡ç« ä¸»è¦è®¨è®ºäº†AIé¢†åŸŸçš„æœ€æ–°å‘å±•ï¼ŒåŒ…æ‹¬ï¼š
1. æ–°æ¨¡å‹å‘å¸ƒå’ŒæŠ€æœ¯çªç ´
2. è¡Œä¸šåº”ç”¨æ¡ˆä¾‹
3. æœªæ¥å‘å±•è¶‹åŠ¿

AIæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå¯¹å„è¡Œå„ä¸šéƒ½äº§ç”Ÿæ·±è¿œå½±å“ã€‚""",
            "keywords": ["AI", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "æŠ€æœ¯çªç ´"],
            "companies_mentioned": [],
        }

    def extract_article_content(self, content: str) -> Dict[str, Any]:
        """
        ä»æ–‡ç« å†…å®¹ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
        
        Args:
            content: æ–‡ç« å†…å®¹
            
        Returns:
            æå–çš„ä¿¡æ¯
        """
        result = {
            "word_count": len(content),
            "paragraph_count": len(content.split("\n\n")),
            "has_numbers": any(c.isdigit() for c in content),
            "has_links": "http" in content.lower(),
        }
        
        return result

    def summarize_content(self, content: str, max_length: int = 200) -> str:
        """
        æ€»ç»“å†…å®¹
        
        Args:
            content: å†…å®¹
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            æ€»ç»“
        """
        # ç®€å•æˆªå–
        if len(content) <= max_length:
            return content
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
        for i in range(min(max_length, len(content) - 1), max_length - 50, -1):
            if content[i] in "ã€‚ï¼ï¼Ÿ.!?\n":
                return content[:i+1]
        
        return content[:max_length] + "..."


class IncrementalExtractor:
    """å¢é‡æå–å™¨ï¼Œæ”¯æŒå¢é‡æ›´æ–°"""

    def __init__(self, extractor: ContentExtractor = None):
        """
        åˆå§‹åŒ–å¢é‡æå–å™¨
        
        Args:
            extractor: å†…å®¹æå–å™¨
        """
        self.extractor = extractor or ContentExtractor()
        self.processed_urls = set()

    def load_processed_urls(self, filepath: str) -> None:
        """
        åŠ è½½å·²å¤„ç†çš„URLåˆ—è¡¨
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
        """
        try:
            import json
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.processed_urls = set(data.get("processed_urls", []))
            print(f"   âœ“ å·²åŠ è½½ {len(self.processed_urls)} ä¸ªå·²å¤„ç†URL")
        except Exception as e:
            print(f"   âš  åŠ è½½å·²å¤„ç†URLå¤±è´¥: {e}")

    def save_processed_urls(self, filepath: str) -> None:
        """
        ä¿å­˜å·²å¤„ç†çš„URLåˆ—è¡¨
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
        """
        try:
            import json
            data = {
                "processed_urls": list(self.processed_urls),
                "last_update": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"   âœ“ å·²ä¿å­˜ {len(self.processed_urls)} ä¸ªå·²å¤„ç†URL")
        except Exception as e:
            print(f"   âš  ä¿å­˜å·²å¤„ç†URLå¤±è´¥: {e}")

    async def extract_new_items(self, items: List[AINewsItem], 
                               show_progress: bool = True) -> List[AINewsItem]:
        """
        åªæå–æ–°é¡¹ç›®
        
        Args:
            items: æ–°é—»åˆ—è¡¨
            show_progress: æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            æ›´æ–°åçš„æ–°é—»åˆ—è¡¨
        """
        new_items = []
        
        for item in items:
            if item.url not in self.processed_urls:
                new_items.append(item)
        
        if show_progress:
            print(f"\nğŸ”„ å¢é‡æå–")
            print(f"   æ–°é¡¹ç›®: {len(new_items)}")
            print(f"   å·²è·³è¿‡: {len(items) - len(new_items)}")
        
        # æå–æ–°é¡¹ç›®
        updated_items = await self.extractor.extract_batch(new_items, show_progress)
        
        # æ›´æ–°å·²å¤„ç†URLé›†åˆ
        for item in new_items:
            self.processed_urls.add(item.url)
        
        return updated_items


async def extraction_demo():
    """æå–æ¼”ç¤º"""
    extractor = ContentExtractor()
    
    # æµ‹è¯•URL
    test_urls = [
        "https://openai.com/blog",
        "https://www.google.com/ai/",
    ]
    
    print("ğŸ§ª å†…å®¹æå–æµ‹è¯•")
    for url in test_urls:
        print(f"\n   æå–: {url}")
        result = await extractor.extract_from_url(url)
        print(f"   æ ‡é¢˜: {result.get('title', 'N/A')}")
        print(f"   æ‘˜è¦: {result.get('summary', 'N/A')[:100]}...")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(extraction_demo())
