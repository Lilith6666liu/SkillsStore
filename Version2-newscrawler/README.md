# AIèµ„è®¯å®æ—¶æŠ“å–ç³»ç»Ÿ

## ğŸ“‹ æ¦‚è¿°

AIèµ„è®¯å®æ—¶æŠ“å–ç³»ç»Ÿæ˜¯ä¸€ä¸ªç»¼åˆæ€§å·¥å…·ï¼Œç”¨äºå®æ—¶æŠ“å–AIé¢†åŸŸçš„åŠ¨æ€ã€æ–°é—»ã€çŸ¥è¯†ã€è®¿è°ˆç­‰å†…å®¹ã€‚è¯¥ç³»ç»Ÿè¦†ç›–å›½å†…å¤–å¤´éƒ¨AIå…¬å¸çš„æœ€æ–°èµ„è®¯ï¼ŒåŒ…æ‹¬OpenAIã€Googleã€Anthropicã€Metaã€ç™¾åº¦ã€é˜¿é‡Œå·´å·´ã€è…¾è®¯ã€å­—èŠ‚è·³åŠ¨ã€æ™ºè°±AIã€æœˆä¹‹æš—é¢ç­‰ã€‚

## âœ¨ åŠŸèƒ½ç‰¹æ€§

- **å®æ—¶æŠ“å–**ï¼šå¿«é€Ÿè·å–æœ€æ–°AIèµ„è®¯
- **å¤šæºèšåˆ**ï¼šæ•´åˆå¤šä¸ªä¿¡æ¯æºï¼ŒåŒ…æ‹¬å®˜æ–¹åšå®¢ã€ç§‘æŠ€åª’ä½“ã€å­¦æœ¯è®ºæ–‡ç­‰
- **æ™ºèƒ½åˆ†ç±»**ï¼šè‡ªåŠ¨å°†èµ„è®¯åˆ†ç±»ä¸ºæ–°é—»ã€äº§å“å‘å¸ƒã€æŠ€æœ¯è§£è¯»ã€å­¦æœ¯çªç ´ã€äººç‰©è®¿è°ˆç­‰
- **ä¸­è‹±æ–‡æ”¯æŒ**ï¼šæ”¯æŒå›½å†…å¤–å†…å®¹åŒºåˆ†å’ŒåŒè¯­æœç´¢
- **æœç´¢ç­›é€‰**ï¼šæ”¯æŒæŒ‰å…³é”®è¯ã€å…¬å¸ã€ç±»åˆ«ã€æ—¶é—´èŒƒå›´ç­‰ç­›é€‰
- **æ•°æ®å¯¼å‡º**ï¼šæ”¯æŒJSONå’ŒCSVæ ¼å¼å¯¼å‡º
- **å¢é‡æ›´æ–°**ï¼šè‡ªåŠ¨è·³è¿‡å·²æŠ“å–å†…å®¹ï¼Œé¿å…é‡å¤

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### åŸºæœ¬ä½¿ç”¨

#### 1. è·å–æœ€æ–°AIèµ„è®¯

```bash
python main.py --mode latest --days 7 --limit 50
```

#### 2. è·å–ç‰¹å®šå…¬å¸åŠ¨æ€

```bash
python main.py --mode company --companies OpenAI Google --days 7
```

#### 3. è·å–ç‰¹å®šç±»åˆ«èµ„è®¯

```bash
python main.py --mode category --category research --days 7
```

#### 4. è·å–å›½é™…AIèµ„è®¯

```bash
python main.py --mode international --days 7
```

#### 5. è·å–å›½å†…AIèµ„è®¯

```bash
python main.py --mode domestic --days 7
```

#### 6. è‡ªå®šä¹‰æœç´¢

```bash
python main.py --mode search --keywords "GPT-5" "Claude 3" --days 7
```

#### 7. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š

```bash
python main.py --mode report --days 7
```

## ğŸ“– Python API ä½¿ç”¨

### åŸºç¡€ç”¨æ³•

```python
import asyncio
from src.main import AINewsScraper

async def main():
    # åˆ›å»ºæŠ“å–å™¨
    scraper = AINewsScraper()
    
    # è·å–æœ€æ–°èµ„è®¯
    items = await scraper.fetch_latest_news(days=7, limit=50)
    
    # æ˜¾ç¤ºç»Ÿè®¡æŠ¥å‘Š
    scraper.display_report(items)
    
    # æ˜¾ç¤ºæ–°é—»åˆ—è¡¨
    scraper.display_news(items, max_items=20)
    
    # å¯¼å‡ºæ•°æ®
    scraper.export_data(items, format="json")
    
    return items

# è¿è¡Œ
items = asyncio.run(main())
```

### é«˜çº§ç”¨æ³•

```python
import asyncio
from src.main import AINewsScraper

async def advanced_demo():
    scraper = AINewsScraper()
    
    # 1. æŒ‰å…¬å¸è·å–åŠ¨æ€
    openai_news = await scraper.fetch_by_company(
        companies=["OpenAI", "Anthropic"],
        days=7,
        limit=20
    )
    
    # 2. æŒ‰ç±»åˆ«è·å–èµ„è®¯
    research_news = await scraper.fetch_by_category(
        category="research",
        days=30,
        limit=20
    )
    
    # 3. è‡ªå®šä¹‰æœç´¢
    custom_news = await scraper.custom_search(
        keywords=["å¤šæ¨¡æ€", "è§†è§‰è¯­è¨€æ¨¡å‹"],
        days=14,
        limit=30
    )
    
    # 4. æå–è¯¦ç»†å†…å®¹
    detailed_news = await scraper.fetch_latest_news(
        days=7,
        limit=10,
        extract_content=True,  # æå–è¯¦ç»†å†…å®¹
        save=True
    )
    
    return openai_news, research_news, custom_news, detailed_news

items = asyncio.run(advanced_demo())
```

### ä½¿ç”¨å„ä¸ªæ¨¡å—

#### æœç´¢æ¨¡å—

```python
from src.searcher import NewsSearcher
import asyncio

async def search_demo():
    searcher = NewsSearcher()
    
    # æœç´¢æœ€æ–°èµ„è®¯
    result = await searcher.search_latest_news(days=7, limit=20)
    print(f"æ‰¾åˆ° {result.total_results} æ¡æ–°é—»")
    
    # æŒ‰å…¬å¸æœç´¢
    result = await searcher.search_by_companies(["OpenAI", "Google"], days=7)
    
    # æŒ‰ç±»åˆ«æœç´¢
    result = await searcher.search_by_category("research", days=30)
    
    # æœç´¢å›½é™…æ–°é—»
    result = await searcher.search_international_news(days=7)
    
    # æœç´¢å›½å†…æ–°é—»
    result = await scraper.search_domestic_news(days=7)
    
    return result.items

items = asyncio.run(search_demo())
```

#### æ•°æ®å¤„ç†æ¨¡å—

```python
from src.processor import NewsProcessor
from src.models import AINewsItem

def processing_demo():
    processor = NewsProcessor()
    
    # å‡è®¾å·²æœ‰æ–°é—»åˆ—è¡¨
    items = []
    
    # å¤„ç†æ–°é—»
    processed_items = processor.process_items(items)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = processor.generate_report(processed_items, days=7)
    
    print(f"æ€»æ–°é—»æ•°: {report.total_news}")
    print(f"å›½é™…æ–°é—»: {report.international_count}")
    print(f"å›½å†…æ–°é—»: {report.domestic_count}")
    
    # è¿‡æ»¤
    filtered = processor.filter_by_date(items, days=7)
    filtered = processor.filter_by_category(items, ["news", "product"])
    filtered = processor.filter_by_source_type(items, "international")
    
    # æ’åº
    sorted_by_importance = processor.sort_by_importance(items)
    sorted_by_date = processor.sort_by_date(items)
    
    # åˆ†ç»„
    by_category = processor.group_by_category(items)
    by_company = processor.group_by_company(items)
    
    # ä¿å­˜
    processor.save_processed_data(items)
    
    return processed_items
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
ai-news-scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py          # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ config.py            # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ models.py            # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ searcher.py          # æœç´¢æ¨¡å—
â”‚   â”œâ”€â”€ extractor.py         # å†…å®¹æå–æ¨¡å—
â”‚   â”œâ”€â”€ processor.py         # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â””â”€â”€ main.py              # ä¸»ç¨‹åº
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ by_category/     # æŒ‰ç±»åˆ«åˆ†ç±»
â”‚   â”‚   â””â”€â”€ by_company/      # æŒ‰å…¬å¸åˆ†ç±»
â”‚   â””â”€â”€ index/               # ç´¢å¼•æ–‡ä»¶
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ research_plan_ai_news_scraper.md
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_usage.py       # åŸºç¡€ç”¨æ³•ç¤ºä¾‹
â”‚   â””â”€â”€ advanced_usage.py    # é«˜çº§ç”¨æ³•ç¤ºä¾‹
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ é…ç½®è¯´æ˜

### æœç´¢é…ç½® (SEARCH_CONFIG)

```python
SEARCH_CONFIG = {
    "max_results_per_query": 10,    # æ¯ä¸ªæœç´¢å…³é”®è¯è¿”å›çš„æœ€å¤§ç»“æœæ•°
    "max_concurrent_searches": 3,   # æœ€å¤§å¹¶å‘æœç´¢æ•°
    "timeout": 30,                   # æœç´¢è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "retry_times": 2,                # é‡è¯•æ¬¡æ•°
}
```

### å†…å®¹æå–é…ç½® (EXTRACT_CONFIG)

```python
EXTRACT_CONFIG = {
    "max_content_length": 5000,     # æå–å†…å®¹çš„æœ€å¤§é•¿åº¦
    "timeout": 15,                   # æå–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "retry_times": 2,                # é‡è¯•æ¬¡æ•°
    "batch_size": 5,                 # æ‰¹é‡æå–å¤§å°
}
```

### ç±»åˆ«é…ç½® (CATEGORIES)

ç³»ç»Ÿæ”¯æŒä»¥ä¸‹ç±»åˆ«ï¼š

- `news` - æ–°é—»
- `product` - äº§å“å‘å¸ƒ
- `technical` - æŠ€æœ¯è§£è¯»
- `research` - å­¦æœ¯çªç ´
- `interview` - äººç‰©è®¿è°ˆ
- `opinion` - è§‚ç‚¹åˆ†æ

### å…¬å¸é…ç½® (COMPANIES)

ç³»ç»Ÿæ”¯æŒä»¥ä¸‹å…¬å¸ï¼š

**å›½é™…å…¬å¸**ï¼š
- OpenAI
- Google
- Anthropic
- Meta
- Microsoft
- Apple
- Amazon
- NVIDIA

**å›½å†…å…¬å¸**ï¼š
- ç™¾åº¦
- é˜¿é‡Œå·´å·´
- è…¾è®¯
- å­—èŠ‚è·³åŠ¨
- æ™ºè°±AI
- æœˆä¹‹æš—é¢
- åä¸º
- ç§‘å¤§è®¯é£

## ğŸ“Š æ•°æ®æ¨¡å‹

### AINewsItem

```python
{
    "id": "unique_id",
    "title": "æ–°é—»æ ‡é¢˜",
    "source": "æ¥æºåç§°",
    "source_type": "international|domestic",
    "category": "news|product|technical|research|interview|opinion",
    "publish_time": "å‘å¸ƒæ—¶é—´",
    "url": "åŸæ–‡é“¾æ¥",
    "summary": "æ‘˜è¦",
    "content": "è¯¦ç»†å†…å®¹ï¼ˆå¯é€‰ï¼‰",
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
    "fetch_time": "æŠ“å–æ—¶é—´",
    "companies": ["OpenAI", "Google"],
    "language": "zh|en|mixed",
    "importance": 1-10
}
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### å¢é‡æ›´æ–°

```python
from src.extractor import IncrementalExtractor

async def incremental_demo():
    extractor = IncrementalExtractor()
    
    # åŠ è½½å·²å¤„ç†URL
    extractor.load_processed_urls("data/index/processed_urls.json")
    
    # å¢é‡æå–ï¼ˆåªæå–æ–°å†…å®¹ï¼‰
    items = await extractor.extract_new_items(all_items, show_progress=True)
    
    # ä¿å­˜å·²å¤„ç†URL
    extractor.save_processed_urls("data/index/processed_urls.json")
    
    return items
```

### æ•°æ®æœç´¢

```python
from src.processor import DataSearcher

def search_demo():
    searcher = DataSearcher()
    
    # åŠ è½½æ•°æ®
    items = searcher.load_latest()
    
    # æœç´¢
    results = searcher.search_items(
        items,
        query="GPT",                    # å…³é”®è¯æœç´¢
        categories=["news", "product"], # ç±»åˆ«è¿‡æ»¤
        companies=["OpenAI"],           # å…¬å¸è¿‡æ»¤
        source_type="international",    # æ¥æºç±»å‹
        days=7                          # æ—¥æœŸè¿‡æ»¤
    )
    
    return results
```

## ğŸ“ å‘½ä»¤è¡Œå‚æ•°

```
--mode            è¿è¡Œæ¨¡å¼ (latest|company|category|international|domestic|search|demo|report)
--keywords        æœç´¢å…³é”®è¯
--companies       æŒ‡å®šå…¬å¸
--category        æ–°é—»ç±»åˆ«
--days            æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤7å¤©
--limit           æœ€å¤§æ•°é‡é™åˆ¶ï¼Œé»˜è®¤50
--extract         æ˜¯å¦æå–è¯¦ç»†å†…å®¹
--no-save         ä¸ä¿å­˜æ•°æ®
--output          è¾“å‡ºæ ¼å¼ (json|csv)
--display         æ˜¾ç¤ºå†…å®¹ (news|report|all)
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **APIé™åˆ¶**ï¼šç³»ç»Ÿä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤ºï¼Œå®é™…ä½¿ç”¨éœ€è¦é…ç½®MCPå·¥å…·
2. **é¢‘ç‡é™åˆ¶**ï¼šè¯·å‹¿è¿‡äºé¢‘ç¹æŠ“å–ï¼Œå»ºè®®æ¯æ¬¡é—´éš”è‡³å°‘5åˆ†é’Ÿ
3. **æ•°æ®éªŒè¯**ï¼šæŠ“å–çš„æ•°æ®éœ€è¦éªŒè¯ï¼Œå»ºè®®äººå·¥å®¡æ ¸
4. **å­˜å‚¨ç©ºé—´**ï¼šå¤§é‡æŠ“å–ä¼šå ç”¨å­˜å‚¨ç©ºé—´ï¼Œå»ºè®®å®šæœŸæ¸…ç†

## ğŸ“¦ ä¾èµ–

- Python 3.8+
- å¼‚æ­¥æ”¯æŒ (asyncio)
- MCPå·¥å…·ï¼ˆå¯é€‰ï¼‰

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ‘¨â€ğŸ’» ä½œè€…

Matrix Agent

---

åˆ›å»ºæ—¶é—´: 2026-01-22
ç‰ˆæœ¬: 1.0.0
